2024-11-24 16:50:36,370:INFO:PyCaret ClassificationExperiment
2024-11-24 16:50:36,371:INFO:Logging name: clf-default-name
2024-11-24 16:50:36,372:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-24 16:50:36,373:INFO:version 3.3.2
2024-11-24 16:50:36,374:INFO:Initializing setup()
2024-11-24 16:50:36,374:INFO:self.USI: 9514
2024-11-24 16:50:36,375:INFO:self._variable_keys: {'gpu_n_jobs_param', 'is_multiclass', 'fix_imbalance', 'fold_groups_param', 'USI', 'data', 'exp_id', '_ml_usecase', 'html_param', 'y_train', 'pipeline', '_available_plots', 'exp_name_log', 'gpu_param', 'fold_generator', 'X_test', 'memory', 'X_train', 'fold_shuffle_param', 'log_plots_param', 'idx', 'y', 'logging_param', 'y_test', 'X', 'seed', 'n_jobs_param', 'target_param'}
2024-11-24 16:50:36,376:INFO:Checking environment
2024-11-24 16:50:36,377:INFO:python_version: 3.10.12
2024-11-24 16:50:36,378:INFO:python_build: ('main', 'Nov  6 2024 20:22:13')
2024-11-24 16:50:36,378:INFO:machine: x86_64
2024-11-24 16:50:36,378:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:50:36,380:INFO:Memory: svmem(total=8156200960, available=6253764608, percent=23.3, used=1578516480, free=5686304768, active=147230720, inactive=1542664192, buffers=5087232, cached=886292480, shared=16797696, slab=451735552)
2024-11-24 16:50:36,382:INFO:Physical Core: 10
2024-11-24 16:50:36,383:INFO:Logical Core: 20
2024-11-24 16:50:36,384:INFO:Checking libraries
2024-11-24 16:50:36,385:INFO:System:
2024-11-24 16:50:36,386:INFO:    python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
2024-11-24 16:50:36,387:INFO:executable: /usr/bin/python3
2024-11-24 16:50:36,387:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:50:36,388:INFO:PyCaret required dependencies:
2024-11-24 16:50:36,514:INFO:                 pip: 22.0.2
2024-11-24 16:50:36,514:INFO:          setuptools: 59.6.0
2024-11-24 16:50:36,515:INFO:             pycaret: 3.3.2
2024-11-24 16:50:36,515:INFO:             IPython: 8.29.0
2024-11-24 16:50:36,516:INFO:          ipywidgets: 8.1.5
2024-11-24 16:50:36,516:INFO:                tqdm: 4.67.0
2024-11-24 16:50:36,516:INFO:               numpy: 1.26.4
2024-11-24 16:50:36,517:INFO:              pandas: 2.1.4
2024-11-24 16:50:36,517:INFO:              jinja2: 3.1.4
2024-11-24 16:50:36,517:INFO:               scipy: 1.11.4
2024-11-24 16:50:36,518:INFO:              joblib: 1.3.2
2024-11-24 16:50:36,518:INFO:             sklearn: 1.4.2
2024-11-24 16:50:36,519:INFO:                pyod: 2.0.2
2024-11-24 16:50:36,519:INFO:            imblearn: 0.12.4
2024-11-24 16:50:36,519:INFO:   category_encoders: 2.6.4
2024-11-24 16:50:36,520:INFO:            lightgbm: 4.5.0
2024-11-24 16:50:36,520:INFO:               numba: 0.60.0
2024-11-24 16:50:36,521:INFO:            requests: 2.32.3
2024-11-24 16:50:36,521:INFO:          matplotlib: 3.7.5
2024-11-24 16:50:36,522:INFO:          scikitplot: 0.3.7
2024-11-24 16:50:36,522:INFO:         yellowbrick: 1.5
2024-11-24 16:50:36,522:INFO:              plotly: 5.24.1
2024-11-24 16:50:36,523:INFO:    plotly-resampler: Not installed
2024-11-24 16:50:36,523:INFO:             kaleido: 0.2.1
2024-11-24 16:50:36,523:INFO:           schemdraw: 0.15
2024-11-24 16:50:36,524:INFO:         statsmodels: 0.14.4
2024-11-24 16:50:36,524:INFO:              sktime: 0.26.0
2024-11-24 16:50:36,524:INFO:               tbats: 1.1.3
2024-11-24 16:50:36,524:INFO:            pmdarima: 2.0.4
2024-11-24 16:50:36,525:INFO:              psutil: 6.1.0
2024-11-24 16:50:36,525:INFO:          markupsafe: 3.0.2
2024-11-24 16:50:36,526:INFO:             pickle5: Not installed
2024-11-24 16:50:36,526:INFO:         cloudpickle: 3.1.0
2024-11-24 16:50:36,526:INFO:         deprecation: 2.1.0
2024-11-24 16:50:36,527:INFO:              xxhash: 3.5.0
2024-11-24 16:50:36,527:INFO:           wurlitzer: 3.1.1
2024-11-24 16:50:36,528:INFO:PyCaret optional dependencies:
2024-11-24 16:50:36,570:INFO:                shap: Not installed
2024-11-24 16:50:36,570:INFO:           interpret: Not installed
2024-11-24 16:50:36,570:INFO:                umap: Not installed
2024-11-24 16:50:36,571:INFO:     ydata_profiling: Not installed
2024-11-24 16:50:36,571:INFO:  explainerdashboard: Not installed
2024-11-24 16:50:36,571:INFO:             autoviz: Not installed
2024-11-24 16:50:36,572:INFO:           fairlearn: Not installed
2024-11-24 16:50:36,572:INFO:          deepchecks: Not installed
2024-11-24 16:50:36,572:INFO:             xgboost: Not installed
2024-11-24 16:50:36,572:INFO:            catboost: Not installed
2024-11-24 16:50:36,573:INFO:              kmodes: Not installed
2024-11-24 16:50:36,573:INFO:             mlxtend: Not installed
2024-11-24 16:50:36,573:INFO:       statsforecast: Not installed
2024-11-24 16:50:36,573:INFO:        tune_sklearn: Not installed
2024-11-24 16:50:36,574:INFO:                 ray: Not installed
2024-11-24 16:50:36,574:INFO:            hyperopt: Not installed
2024-11-24 16:50:36,574:INFO:              optuna: Not installed
2024-11-24 16:50:36,575:INFO:               skopt: Not installed
2024-11-24 16:50:36,575:INFO:              mlflow: Not installed
2024-11-24 16:50:36,575:INFO:              gradio: Not installed
2024-11-24 16:50:36,576:INFO:             fastapi: Not installed
2024-11-24 16:50:36,576:INFO:             uvicorn: Not installed
2024-11-24 16:50:36,576:INFO:              m2cgen: Not installed
2024-11-24 16:50:36,577:INFO:           evidently: Not installed
2024-11-24 16:50:36,577:INFO:               fugue: Not installed
2024-11-24 16:50:36,577:INFO:           streamlit: Not installed
2024-11-24 16:50:36,578:INFO:             prophet: Not installed
2024-11-24 16:50:36,578:INFO:None
2024-11-24 16:50:36,578:INFO:Set up data.
2024-11-24 16:50:36,601:INFO:Set up folding strategy.
2024-11-24 16:50:36,602:INFO:Set up train/test split.
2024-11-24 16:50:36,626:INFO:Set up index.
2024-11-24 16:50:36,628:INFO:Assigning column types.
2024-11-24 16:50:36,636:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-24 16:50:36,663:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:50:36,667:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:50:36,687:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,688:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,738:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:50:36,750:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:50:36,767:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,768:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,770:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-24 16:50:36,797:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:50:36,816:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,817:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,847:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:50:36,864:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,866:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,867:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-24 16:50:36,916:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,920:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:36,976:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:37,001:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:37,060:INFO:Preparing preprocessing pipeline...
2024-11-24 16:50:37,097:INFO:Set up simple imputation.
2024-11-24 16:50:37,125:INFO:Set up encoding of ordinal features.
2024-11-24 16:50:37,140:INFO:Set up encoding of categorical features.
2024-11-24 16:50:37,714:INFO:Finished creating preprocessing pipeline.
2024-11-24 16:50:37,734:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Quantity', 'Fee',
                                             'VideoAmt', 'PhotoAmt'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=...
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Breed1', 'Breed2'],
                                    transformer=TargetEncoder(cols=[],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-24 16:50:37,758:INFO:Creating final display dataframe.
2024-11-24 16:50:38,297:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target     AdoptionSpeed
2                   Target type        Multiclass
3           Original data shape       (14993, 21)
4        Transformed data shape       (14993, 66)
5   Transformed train set shape       (10495, 66)
6    Transformed test set shape        (4498, 66)
7              Numeric features                 5
8          Categorical features                14
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              9514
2024-11-24 16:50:38,366:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:38,368:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:38,424:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:38,426:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:50:38,432:INFO:setup() successfully completed in 2.08s...............
2024-11-24 16:55:00,713:INFO:PyCaret ClassificationExperiment
2024-11-24 16:55:00,713:INFO:Logging name: clf-default-name
2024-11-24 16:55:00,714:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-24 16:55:00,714:INFO:version 3.3.2
2024-11-24 16:55:00,714:INFO:Initializing setup()
2024-11-24 16:55:00,715:INFO:self.USI: 8dd5
2024-11-24 16:55:00,715:INFO:self._variable_keys: {'gpu_n_jobs_param', 'is_multiclass', 'fix_imbalance', 'fold_groups_param', 'USI', 'data', 'exp_id', '_ml_usecase', 'html_param', 'y_train', 'pipeline', '_available_plots', 'exp_name_log', 'gpu_param', 'fold_generator', 'X_test', 'memory', 'X_train', 'fold_shuffle_param', 'log_plots_param', 'idx', 'y', 'logging_param', 'y_test', 'X', 'seed', 'n_jobs_param', 'target_param'}
2024-11-24 16:55:00,715:INFO:Checking environment
2024-11-24 16:55:00,716:INFO:python_version: 3.10.12
2024-11-24 16:55:00,716:INFO:python_build: ('main', 'Nov  6 2024 20:22:13')
2024-11-24 16:55:00,716:INFO:machine: x86_64
2024-11-24 16:55:00,717:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:55:00,717:INFO:Memory: svmem(total=8156200960, available=6221422592, percent=23.7, used=1610842112, free=5627682816, active=149999616, inactive=1598533632, buffers=7725056, cached=909950976, shared=16797696, slab=451874816)
2024-11-24 16:55:00,719:INFO:Physical Core: 10
2024-11-24 16:55:00,719:INFO:Logical Core: 20
2024-11-24 16:55:00,720:INFO:Checking libraries
2024-11-24 16:55:00,720:INFO:System:
2024-11-24 16:55:00,720:INFO:    python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
2024-11-24 16:55:00,721:INFO:executable: /usr/bin/python3
2024-11-24 16:55:00,721:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:55:00,722:INFO:PyCaret required dependencies:
2024-11-24 16:55:00,722:INFO:                 pip: 22.0.2
2024-11-24 16:55:00,722:INFO:          setuptools: 59.6.0
2024-11-24 16:55:00,723:INFO:             pycaret: 3.3.2
2024-11-24 16:55:00,723:INFO:             IPython: 8.29.0
2024-11-24 16:55:00,724:INFO:          ipywidgets: 8.1.5
2024-11-24 16:55:00,724:INFO:                tqdm: 4.67.0
2024-11-24 16:55:00,725:INFO:               numpy: 1.26.4
2024-11-24 16:55:00,725:INFO:              pandas: 2.1.4
2024-11-24 16:55:00,726:INFO:              jinja2: 3.1.4
2024-11-24 16:55:00,726:INFO:               scipy: 1.11.4
2024-11-24 16:55:00,727:INFO:              joblib: 1.3.2
2024-11-24 16:55:00,727:INFO:             sklearn: 1.4.2
2024-11-24 16:55:00,728:INFO:                pyod: 2.0.2
2024-11-24 16:55:00,728:INFO:            imblearn: 0.12.4
2024-11-24 16:55:00,729:INFO:   category_encoders: 2.6.4
2024-11-24 16:55:00,729:INFO:            lightgbm: 4.5.0
2024-11-24 16:55:00,730:INFO:               numba: 0.60.0
2024-11-24 16:55:00,730:INFO:            requests: 2.32.3
2024-11-24 16:55:00,731:INFO:          matplotlib: 3.7.5
2024-11-24 16:55:00,732:INFO:          scikitplot: 0.3.7
2024-11-24 16:55:00,732:INFO:         yellowbrick: 1.5
2024-11-24 16:55:00,733:INFO:              plotly: 5.24.1
2024-11-24 16:55:00,733:INFO:    plotly-resampler: Not installed
2024-11-24 16:55:00,734:INFO:             kaleido: 0.2.1
2024-11-24 16:55:00,735:INFO:           schemdraw: 0.15
2024-11-24 16:55:00,736:INFO:         statsmodels: 0.14.4
2024-11-24 16:55:00,736:INFO:              sktime: 0.26.0
2024-11-24 16:55:00,737:INFO:               tbats: 1.1.3
2024-11-24 16:55:00,738:INFO:            pmdarima: 2.0.4
2024-11-24 16:55:00,739:INFO:              psutil: 6.1.0
2024-11-24 16:55:00,740:INFO:          markupsafe: 3.0.2
2024-11-24 16:55:00,741:INFO:             pickle5: Not installed
2024-11-24 16:55:00,742:INFO:         cloudpickle: 3.1.0
2024-11-24 16:55:00,743:INFO:         deprecation: 2.1.0
2024-11-24 16:55:00,744:INFO:              xxhash: 3.5.0
2024-11-24 16:55:00,745:INFO:           wurlitzer: 3.1.1
2024-11-24 16:55:00,746:INFO:PyCaret optional dependencies:
2024-11-24 16:55:00,747:INFO:                shap: Not installed
2024-11-24 16:55:00,748:INFO:           interpret: Not installed
2024-11-24 16:55:00,749:INFO:                umap: Not installed
2024-11-24 16:55:00,749:INFO:     ydata_profiling: Not installed
2024-11-24 16:55:00,750:INFO:  explainerdashboard: Not installed
2024-11-24 16:55:00,751:INFO:             autoviz: Not installed
2024-11-24 16:55:00,751:INFO:           fairlearn: Not installed
2024-11-24 16:55:00,752:INFO:          deepchecks: Not installed
2024-11-24 16:55:00,753:INFO:             xgboost: Not installed
2024-11-24 16:55:00,754:INFO:            catboost: Not installed
2024-11-24 16:55:00,754:INFO:              kmodes: Not installed
2024-11-24 16:55:00,755:INFO:             mlxtend: Not installed
2024-11-24 16:55:00,756:INFO:       statsforecast: Not installed
2024-11-24 16:55:00,756:INFO:        tune_sklearn: Not installed
2024-11-24 16:55:00,757:INFO:                 ray: Not installed
2024-11-24 16:55:00,758:INFO:            hyperopt: Not installed
2024-11-24 16:55:00,759:INFO:              optuna: Not installed
2024-11-24 16:55:00,760:INFO:               skopt: Not installed
2024-11-24 16:55:00,761:INFO:              mlflow: Not installed
2024-11-24 16:55:00,762:INFO:              gradio: Not installed
2024-11-24 16:55:00,762:INFO:             fastapi: Not installed
2024-11-24 16:55:00,763:INFO:             uvicorn: Not installed
2024-11-24 16:55:00,764:INFO:              m2cgen: Not installed
2024-11-24 16:55:00,764:INFO:           evidently: Not installed
2024-11-24 16:55:00,765:INFO:               fugue: Not installed
2024-11-24 16:55:00,765:INFO:           streamlit: Not installed
2024-11-24 16:55:00,766:INFO:             prophet: Not installed
2024-11-24 16:55:00,767:INFO:None
2024-11-24 16:55:00,767:INFO:Set up data.
2024-11-24 16:55:00,779:INFO:Set up folding strategy.
2024-11-24 16:55:00,780:INFO:Set up train/test split.
2024-11-24 16:55:00,792:INFO:Set up index.
2024-11-24 16:55:00,794:INFO:Assigning column types.
2024-11-24 16:55:00,800:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-24 16:55:00,828:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:55:00,829:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:55:00,847:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:00,849:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:00,874:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:55:00,876:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:55:00,893:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:00,893:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:00,894:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-24 16:55:00,924:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:55:00,941:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:00,942:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:00,971:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:55:00,988:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:00,989:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:00,990:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-24 16:55:01,039:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:01,040:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:01,090:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:01,091:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:01,092:INFO:Preparing preprocessing pipeline...
2024-11-24 16:55:01,094:INFO:Set up simple imputation.
2024-11-24 16:55:01,100:INFO:Set up encoding of ordinal features.
2024-11-24 16:55:01,102:INFO:Set up encoding of categorical features.
2024-11-24 16:55:01,457:INFO:Finished creating preprocessing pipeline.
2024-11-24 16:55:01,480:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Quantity', 'Fee',
                                             'VideoAmt', 'PhotoAmt'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=...
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Breed1', 'Breed2'],
                                    transformer=TargetEncoder(cols=[],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-24 16:55:01,481:INFO:Creating final display dataframe.
2024-11-24 16:55:02,148:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target     AdoptionSpeed
2                   Target type        Multiclass
3           Original data shape       (14993, 21)
4        Transformed data shape       (14993, 66)
5   Transformed train set shape       (10495, 66)
6    Transformed test set shape        (4498, 66)
7              Numeric features                 5
8          Categorical features                14
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              8dd5
2024-11-24 16:55:02,203:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:02,204:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:02,258:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:02,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:02,260:INFO:setup() successfully completed in 1.55s...............
2024-11-24 16:55:48,751:INFO:PyCaret ClassificationExperiment
2024-11-24 16:55:48,751:INFO:Logging name: clf-default-name
2024-11-24 16:55:48,752:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-24 16:55:48,752:INFO:version 3.3.2
2024-11-24 16:55:48,753:INFO:Initializing setup()
2024-11-24 16:55:48,753:INFO:self.USI: 69f4
2024-11-24 16:55:48,754:INFO:self._variable_keys: {'gpu_n_jobs_param', 'is_multiclass', 'fix_imbalance', 'fold_groups_param', 'USI', 'data', 'exp_id', '_ml_usecase', 'html_param', 'y_train', 'pipeline', '_available_plots', 'exp_name_log', 'gpu_param', 'fold_generator', 'X_test', 'memory', 'X_train', 'fold_shuffle_param', 'log_plots_param', 'idx', 'y', 'logging_param', 'y_test', 'X', 'seed', 'n_jobs_param', 'target_param'}
2024-11-24 16:55:48,754:INFO:Checking environment
2024-11-24 16:55:48,754:INFO:python_version: 3.10.12
2024-11-24 16:55:48,754:INFO:python_build: ('main', 'Nov  6 2024 20:22:13')
2024-11-24 16:55:48,755:INFO:machine: x86_64
2024-11-24 16:55:48,755:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:55:48,756:INFO:Memory: svmem(total=8156200960, available=6217474048, percent=23.8, used=1614798848, free=5609234432, active=150097920, inactive=1617227776, buffers=7802880, cached=924364800, shared=16797696, slab=451874816)
2024-11-24 16:55:48,757:INFO:Physical Core: 10
2024-11-24 16:55:48,757:INFO:Logical Core: 20
2024-11-24 16:55:48,758:INFO:Checking libraries
2024-11-24 16:55:48,758:INFO:System:
2024-11-24 16:55:48,759:INFO:    python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
2024-11-24 16:55:48,759:INFO:executable: /usr/bin/python3
2024-11-24 16:55:48,760:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:55:48,760:INFO:PyCaret required dependencies:
2024-11-24 16:55:48,761:INFO:                 pip: 22.0.2
2024-11-24 16:55:48,762:INFO:          setuptools: 59.6.0
2024-11-24 16:55:48,762:INFO:             pycaret: 3.3.2
2024-11-24 16:55:48,762:INFO:             IPython: 8.29.0
2024-11-24 16:55:48,763:INFO:          ipywidgets: 8.1.5
2024-11-24 16:55:48,763:INFO:                tqdm: 4.67.0
2024-11-24 16:55:48,764:INFO:               numpy: 1.26.4
2024-11-24 16:55:48,764:INFO:              pandas: 2.1.4
2024-11-24 16:55:48,765:INFO:              jinja2: 3.1.4
2024-11-24 16:55:48,765:INFO:               scipy: 1.11.4
2024-11-24 16:55:48,765:INFO:              joblib: 1.3.2
2024-11-24 16:55:48,766:INFO:             sklearn: 1.4.2
2024-11-24 16:55:48,766:INFO:                pyod: 2.0.2
2024-11-24 16:55:48,766:INFO:            imblearn: 0.12.4
2024-11-24 16:55:48,767:INFO:   category_encoders: 2.6.4
2024-11-24 16:55:48,767:INFO:            lightgbm: 4.5.0
2024-11-24 16:55:48,768:INFO:               numba: 0.60.0
2024-11-24 16:55:48,768:INFO:            requests: 2.32.3
2024-11-24 16:55:48,768:INFO:          matplotlib: 3.7.5
2024-11-24 16:55:48,768:INFO:          scikitplot: 0.3.7
2024-11-24 16:55:48,769:INFO:         yellowbrick: 1.5
2024-11-24 16:55:48,769:INFO:              plotly: 5.24.1
2024-11-24 16:55:48,770:INFO:    plotly-resampler: Not installed
2024-11-24 16:55:48,770:INFO:             kaleido: 0.2.1
2024-11-24 16:55:48,770:INFO:           schemdraw: 0.15
2024-11-24 16:55:48,771:INFO:         statsmodels: 0.14.4
2024-11-24 16:55:48,771:INFO:              sktime: 0.26.0
2024-11-24 16:55:48,771:INFO:               tbats: 1.1.3
2024-11-24 16:55:48,772:INFO:            pmdarima: 2.0.4
2024-11-24 16:55:48,772:INFO:              psutil: 6.1.0
2024-11-24 16:55:48,773:INFO:          markupsafe: 3.0.2
2024-11-24 16:55:48,773:INFO:             pickle5: Not installed
2024-11-24 16:55:48,773:INFO:         cloudpickle: 3.1.0
2024-11-24 16:55:48,773:INFO:         deprecation: 2.1.0
2024-11-24 16:55:48,774:INFO:              xxhash: 3.5.0
2024-11-24 16:55:48,774:INFO:           wurlitzer: 3.1.1
2024-11-24 16:55:48,775:INFO:PyCaret optional dependencies:
2024-11-24 16:55:48,775:INFO:                shap: Not installed
2024-11-24 16:55:48,775:INFO:           interpret: Not installed
2024-11-24 16:55:48,776:INFO:                umap: Not installed
2024-11-24 16:55:48,776:INFO:     ydata_profiling: Not installed
2024-11-24 16:55:48,777:INFO:  explainerdashboard: Not installed
2024-11-24 16:55:48,777:INFO:             autoviz: Not installed
2024-11-24 16:55:48,778:INFO:           fairlearn: Not installed
2024-11-24 16:55:48,778:INFO:          deepchecks: Not installed
2024-11-24 16:55:48,779:INFO:             xgboost: Not installed
2024-11-24 16:55:48,779:INFO:            catboost: Not installed
2024-11-24 16:55:48,779:INFO:              kmodes: Not installed
2024-11-24 16:55:48,780:INFO:             mlxtend: Not installed
2024-11-24 16:55:48,780:INFO:       statsforecast: Not installed
2024-11-24 16:55:48,781:INFO:        tune_sklearn: Not installed
2024-11-24 16:55:48,781:INFO:                 ray: Not installed
2024-11-24 16:55:48,782:INFO:            hyperopt: Not installed
2024-11-24 16:55:48,782:INFO:              optuna: Not installed
2024-11-24 16:55:48,783:INFO:               skopt: Not installed
2024-11-24 16:55:48,783:INFO:              mlflow: Not installed
2024-11-24 16:55:48,784:INFO:              gradio: Not installed
2024-11-24 16:55:48,784:INFO:             fastapi: Not installed
2024-11-24 16:55:48,785:INFO:             uvicorn: Not installed
2024-11-24 16:55:48,785:INFO:              m2cgen: Not installed
2024-11-24 16:55:48,786:INFO:           evidently: Not installed
2024-11-24 16:55:48,786:INFO:               fugue: Not installed
2024-11-24 16:55:48,787:INFO:           streamlit: Not installed
2024-11-24 16:55:48,788:INFO:             prophet: Not installed
2024-11-24 16:55:48,788:INFO:None
2024-11-24 16:55:48,789:INFO:Set up data.
2024-11-24 16:55:48,801:INFO:Set up folding strategy.
2024-11-24 16:55:48,802:INFO:Set up train/test split.
2024-11-24 16:55:48,813:INFO:Set up index.
2024-11-24 16:55:48,814:INFO:Assigning column types.
2024-11-24 16:55:48,820:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-24 16:55:48,854:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:55:48,856:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:55:48,876:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:48,877:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:48,903:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:55:48,905:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:55:48,923:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:48,924:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:48,925:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-24 16:55:48,954:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:55:48,972:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:48,973:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:49,000:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:55:49,017:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:49,018:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:49,018:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-24 16:55:49,083:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:49,084:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:49,127:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:49,128:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:49,128:INFO:Preparing preprocessing pipeline...
2024-11-24 16:55:49,130:INFO:Set up simple imputation.
2024-11-24 16:55:49,135:INFO:Set up encoding of ordinal features.
2024-11-24 16:55:49,137:INFO:Set up encoding of categorical features.
2024-11-24 16:55:49,645:INFO:Finished creating preprocessing pipeline.
2024-11-24 16:55:49,679:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Quantity', 'Fee',
                                             'VideoAmt', 'PhotoAmt'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=...
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Breed1', 'Breed2', 'RescuerID'],
                                    transformer=TargetEncoder(cols=['Breed1',
                                                                    'Breed2',
                                                                    'RescuerID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-24 16:55:49,679:INFO:Creating final display dataframe.
2024-11-24 16:55:50,401:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target     AdoptionSpeed
2                   Target type        Multiclass
3           Original data shape       (14993, 21)
4        Transformed data shape       (14993, 66)
5   Transformed train set shape       (10495, 66)
6    Transformed test set shape        (4498, 66)
7              Numeric features                 5
8          Categorical features                15
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              69f4
2024-11-24 16:55:50,474:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:50,475:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:50,522:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:50,522:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:55:50,524:INFO:setup() successfully completed in 1.78s...............
2024-11-24 16:56:38,313:INFO:PyCaret ClassificationExperiment
2024-11-24 16:56:38,314:INFO:Logging name: clf-default-name
2024-11-24 16:56:38,314:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-24 16:56:38,314:INFO:version 3.3.2
2024-11-24 16:56:38,314:INFO:Initializing setup()
2024-11-24 16:56:38,315:INFO:self.USI: cd09
2024-11-24 16:56:38,315:INFO:self._variable_keys: {'gpu_n_jobs_param', 'is_multiclass', 'fix_imbalance', 'fold_groups_param', 'USI', 'data', 'exp_id', '_ml_usecase', 'html_param', 'y_train', 'pipeline', '_available_plots', 'exp_name_log', 'gpu_param', 'fold_generator', 'X_test', 'memory', 'X_train', 'fold_shuffle_param', 'log_plots_param', 'idx', 'y', 'logging_param', 'y_test', 'X', 'seed', 'n_jobs_param', 'target_param'}
2024-11-24 16:56:38,315:INFO:Checking environment
2024-11-24 16:56:38,316:INFO:python_version: 3.10.12
2024-11-24 16:56:38,316:INFO:python_build: ('main', 'Nov  6 2024 20:22:13')
2024-11-24 16:56:38,316:INFO:machine: x86_64
2024-11-24 16:56:38,316:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:56:38,317:INFO:Memory: svmem(total=8156200960, available=6211534848, percent=23.8, used=1620738048, free=5594271744, active=150171648, inactive=1634562048, buffers=7892992, cached=933298176, shared=16797696, slab=452116480)
2024-11-24 16:56:38,318:INFO:Physical Core: 10
2024-11-24 16:56:38,318:INFO:Logical Core: 20
2024-11-24 16:56:38,318:INFO:Checking libraries
2024-11-24 16:56:38,319:INFO:System:
2024-11-24 16:56:38,320:INFO:    python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
2024-11-24 16:56:38,320:INFO:executable: /usr/bin/python3
2024-11-24 16:56:38,320:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:56:38,321:INFO:PyCaret required dependencies:
2024-11-24 16:56:38,321:INFO:                 pip: 22.0.2
2024-11-24 16:56:38,322:INFO:          setuptools: 59.6.0
2024-11-24 16:56:38,322:INFO:             pycaret: 3.3.2
2024-11-24 16:56:38,322:INFO:             IPython: 8.29.0
2024-11-24 16:56:38,322:INFO:          ipywidgets: 8.1.5
2024-11-24 16:56:38,323:INFO:                tqdm: 4.67.0
2024-11-24 16:56:38,323:INFO:               numpy: 1.26.4
2024-11-24 16:56:38,323:INFO:              pandas: 2.1.4
2024-11-24 16:56:38,324:INFO:              jinja2: 3.1.4
2024-11-24 16:56:38,324:INFO:               scipy: 1.11.4
2024-11-24 16:56:38,325:INFO:              joblib: 1.3.2
2024-11-24 16:56:38,325:INFO:             sklearn: 1.4.2
2024-11-24 16:56:38,325:INFO:                pyod: 2.0.2
2024-11-24 16:56:38,325:INFO:            imblearn: 0.12.4
2024-11-24 16:56:38,326:INFO:   category_encoders: 2.6.4
2024-11-24 16:56:38,326:INFO:            lightgbm: 4.5.0
2024-11-24 16:56:38,326:INFO:               numba: 0.60.0
2024-11-24 16:56:38,327:INFO:            requests: 2.32.3
2024-11-24 16:56:38,327:INFO:          matplotlib: 3.7.5
2024-11-24 16:56:38,328:INFO:          scikitplot: 0.3.7
2024-11-24 16:56:38,328:INFO:         yellowbrick: 1.5
2024-11-24 16:56:38,329:INFO:              plotly: 5.24.1
2024-11-24 16:56:38,329:INFO:    plotly-resampler: Not installed
2024-11-24 16:56:38,329:INFO:             kaleido: 0.2.1
2024-11-24 16:56:38,330:INFO:           schemdraw: 0.15
2024-11-24 16:56:38,330:INFO:         statsmodels: 0.14.4
2024-11-24 16:56:38,330:INFO:              sktime: 0.26.0
2024-11-24 16:56:38,331:INFO:               tbats: 1.1.3
2024-11-24 16:56:38,331:INFO:            pmdarima: 2.0.4
2024-11-24 16:56:38,331:INFO:              psutil: 6.1.0
2024-11-24 16:56:38,332:INFO:          markupsafe: 3.0.2
2024-11-24 16:56:38,332:INFO:             pickle5: Not installed
2024-11-24 16:56:38,332:INFO:         cloudpickle: 3.1.0
2024-11-24 16:56:38,333:INFO:         deprecation: 2.1.0
2024-11-24 16:56:38,333:INFO:              xxhash: 3.5.0
2024-11-24 16:56:38,334:INFO:           wurlitzer: 3.1.1
2024-11-24 16:56:38,335:INFO:PyCaret optional dependencies:
2024-11-24 16:56:38,335:INFO:                shap: Not installed
2024-11-24 16:56:38,335:INFO:           interpret: Not installed
2024-11-24 16:56:38,336:INFO:                umap: Not installed
2024-11-24 16:56:38,336:INFO:     ydata_profiling: Not installed
2024-11-24 16:56:38,337:INFO:  explainerdashboard: Not installed
2024-11-24 16:56:38,337:INFO:             autoviz: Not installed
2024-11-24 16:56:38,338:INFO:           fairlearn: Not installed
2024-11-24 16:56:38,338:INFO:          deepchecks: Not installed
2024-11-24 16:56:38,339:INFO:             xgboost: Not installed
2024-11-24 16:56:38,339:INFO:            catboost: Not installed
2024-11-24 16:56:38,340:INFO:              kmodes: Not installed
2024-11-24 16:56:38,340:INFO:             mlxtend: Not installed
2024-11-24 16:56:38,341:INFO:       statsforecast: Not installed
2024-11-24 16:56:38,341:INFO:        tune_sklearn: Not installed
2024-11-24 16:56:38,342:INFO:                 ray: Not installed
2024-11-24 16:56:38,342:INFO:            hyperopt: Not installed
2024-11-24 16:56:38,343:INFO:              optuna: Not installed
2024-11-24 16:56:38,344:INFO:               skopt: Not installed
2024-11-24 16:56:38,344:INFO:              mlflow: Not installed
2024-11-24 16:56:38,345:INFO:              gradio: Not installed
2024-11-24 16:56:38,345:INFO:             fastapi: Not installed
2024-11-24 16:56:38,346:INFO:             uvicorn: Not installed
2024-11-24 16:56:38,346:INFO:              m2cgen: Not installed
2024-11-24 16:56:38,347:INFO:           evidently: Not installed
2024-11-24 16:56:38,348:INFO:               fugue: Not installed
2024-11-24 16:56:38,348:INFO:           streamlit: Not installed
2024-11-24 16:56:38,349:INFO:             prophet: Not installed
2024-11-24 16:56:38,349:INFO:None
2024-11-24 16:56:38,349:INFO:Set up data.
2024-11-24 16:56:38,362:INFO:Set up folding strategy.
2024-11-24 16:56:38,363:INFO:Set up train/test split.
2024-11-24 16:56:38,375:INFO:Set up index.
2024-11-24 16:56:38,376:INFO:Assigning column types.
2024-11-24 16:56:38,381:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-24 16:56:38,414:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:56:38,416:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:56:38,439:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,440:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,467:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:56:38,468:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:56:38,486:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,487:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,488:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-24 16:56:38,517:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:56:38,537:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,538:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,566:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:56:38,585:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,586:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,586:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-24 16:56:38,630:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,631:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,678:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,679:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:38,681:INFO:Preparing preprocessing pipeline...
2024-11-24 16:56:38,682:INFO:Set up simple imputation.
2024-11-24 16:56:38,689:INFO:Set up encoding of ordinal features.
2024-11-24 16:56:38,692:INFO:Set up encoding of categorical features.
2024-11-24 16:56:39,024:INFO:Finished creating preprocessing pipeline.
2024-11-24 16:56:39,043:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Quantity', 'Fee',
                                             'VideoAmt', 'PhotoAmt'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=...
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Breed1', 'Breed2', 'RescuerID'],
                                    transformer=TargetEncoder(cols=['Breed1',
                                                                    'Breed2',
                                                                    'RescuerID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-24 16:56:39,044:INFO:Creating final display dataframe.
2024-11-24 16:56:39,727:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target     AdoptionSpeed
2                   Target type        Multiclass
3           Original data shape       (14993, 21)
4        Transformed data shape       (14993, 66)
5   Transformed train set shape       (10495, 66)
6    Transformed test set shape        (4498, 66)
7              Numeric features                 5
8          Categorical features                15
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              cd09
2024-11-24 16:56:39,803:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:39,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:39,861:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:39,862:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:56:39,864:INFO:setup() successfully completed in 1.56s...............
2024-11-24 16:57:01,044:INFO:PyCaret ClassificationExperiment
2024-11-24 16:57:01,045:INFO:Logging name: clf-default-name
2024-11-24 16:57:01,045:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-24 16:57:01,047:INFO:version 3.3.2
2024-11-24 16:57:01,047:INFO:Initializing setup()
2024-11-24 16:57:01,048:INFO:self.USI: 9ada
2024-11-24 16:57:01,048:INFO:self._variable_keys: {'gpu_n_jobs_param', 'is_multiclass', 'fix_imbalance', 'fold_groups_param', 'USI', 'data', 'exp_id', '_ml_usecase', 'html_param', 'y_train', 'pipeline', '_available_plots', 'exp_name_log', 'gpu_param', 'fold_generator', 'X_test', 'memory', 'X_train', 'fold_shuffle_param', 'log_plots_param', 'idx', 'y', 'logging_param', 'y_test', 'X', 'seed', 'n_jobs_param', 'target_param'}
2024-11-24 16:57:01,049:INFO:Checking environment
2024-11-24 16:57:01,050:INFO:python_version: 3.10.12
2024-11-24 16:57:01,050:INFO:python_build: ('main', 'Nov  6 2024 20:22:13')
2024-11-24 16:57:01,051:INFO:machine: x86_64
2024-11-24 16:57:01,051:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:57:01,052:INFO:Memory: svmem(total=8156200960, available=6206169088, percent=23.9, used=1625767936, free=5511671808, active=156913664, inactive=1707413504, buffers=8200192, cached=1010561024, shared=16797696, slab=452321280)
2024-11-24 16:57:01,054:INFO:Physical Core: 10
2024-11-24 16:57:01,054:INFO:Logical Core: 20
2024-11-24 16:57:01,055:INFO:Checking libraries
2024-11-24 16:57:01,056:INFO:System:
2024-11-24 16:57:01,056:INFO:    python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
2024-11-24 16:57:01,057:INFO:executable: /usr/bin/python3
2024-11-24 16:57:01,057:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 16:57:01,058:INFO:PyCaret required dependencies:
2024-11-24 16:57:01,059:INFO:                 pip: 22.0.2
2024-11-24 16:57:01,060:INFO:          setuptools: 59.6.0
2024-11-24 16:57:01,060:INFO:             pycaret: 3.3.2
2024-11-24 16:57:01,061:INFO:             IPython: 8.29.0
2024-11-24 16:57:01,061:INFO:          ipywidgets: 8.1.5
2024-11-24 16:57:01,062:INFO:                tqdm: 4.67.0
2024-11-24 16:57:01,063:INFO:               numpy: 1.26.4
2024-11-24 16:57:01,063:INFO:              pandas: 2.1.4
2024-11-24 16:57:01,064:INFO:              jinja2: 3.1.4
2024-11-24 16:57:01,064:INFO:               scipy: 1.11.4
2024-11-24 16:57:01,065:INFO:              joblib: 1.3.2
2024-11-24 16:57:01,065:INFO:             sklearn: 1.4.2
2024-11-24 16:57:01,065:INFO:                pyod: 2.0.2
2024-11-24 16:57:01,066:INFO:            imblearn: 0.12.4
2024-11-24 16:57:01,067:INFO:   category_encoders: 2.6.4
2024-11-24 16:57:01,067:INFO:            lightgbm: 4.5.0
2024-11-24 16:57:01,068:INFO:               numba: 0.60.0
2024-11-24 16:57:01,068:INFO:            requests: 2.32.3
2024-11-24 16:57:01,069:INFO:          matplotlib: 3.7.5
2024-11-24 16:57:01,070:INFO:          scikitplot: 0.3.7
2024-11-24 16:57:01,070:INFO:         yellowbrick: 1.5
2024-11-24 16:57:01,071:INFO:              plotly: 5.24.1
2024-11-24 16:57:01,071:INFO:    plotly-resampler: Not installed
2024-11-24 16:57:01,072:INFO:             kaleido: 0.2.1
2024-11-24 16:57:01,072:INFO:           schemdraw: 0.15
2024-11-24 16:57:01,073:INFO:         statsmodels: 0.14.4
2024-11-24 16:57:01,074:INFO:              sktime: 0.26.0
2024-11-24 16:57:01,074:INFO:               tbats: 1.1.3
2024-11-24 16:57:01,075:INFO:            pmdarima: 2.0.4
2024-11-24 16:57:01,076:INFO:              psutil: 6.1.0
2024-11-24 16:57:01,076:INFO:          markupsafe: 3.0.2
2024-11-24 16:57:01,077:INFO:             pickle5: Not installed
2024-11-24 16:57:01,078:INFO:         cloudpickle: 3.1.0
2024-11-24 16:57:01,078:INFO:         deprecation: 2.1.0
2024-11-24 16:57:01,079:INFO:              xxhash: 3.5.0
2024-11-24 16:57:01,080:INFO:           wurlitzer: 3.1.1
2024-11-24 16:57:01,080:INFO:PyCaret optional dependencies:
2024-11-24 16:57:01,081:INFO:                shap: Not installed
2024-11-24 16:57:01,082:INFO:           interpret: Not installed
2024-11-24 16:57:01,082:INFO:                umap: Not installed
2024-11-24 16:57:01,083:INFO:     ydata_profiling: Not installed
2024-11-24 16:57:01,083:INFO:  explainerdashboard: Not installed
2024-11-24 16:57:01,084:INFO:             autoviz: Not installed
2024-11-24 16:57:01,084:INFO:           fairlearn: Not installed
2024-11-24 16:57:01,085:INFO:          deepchecks: Not installed
2024-11-24 16:57:01,085:INFO:             xgboost: Not installed
2024-11-24 16:57:01,086:INFO:            catboost: Not installed
2024-11-24 16:57:01,087:INFO:              kmodes: Not installed
2024-11-24 16:57:01,087:INFO:             mlxtend: Not installed
2024-11-24 16:57:01,088:INFO:       statsforecast: Not installed
2024-11-24 16:57:01,089:INFO:        tune_sklearn: Not installed
2024-11-24 16:57:01,089:INFO:                 ray: Not installed
2024-11-24 16:57:01,090:INFO:            hyperopt: Not installed
2024-11-24 16:57:01,091:INFO:              optuna: Not installed
2024-11-24 16:57:01,091:INFO:               skopt: Not installed
2024-11-24 16:57:01,092:INFO:              mlflow: Not installed
2024-11-24 16:57:01,092:INFO:              gradio: Not installed
2024-11-24 16:57:01,107:INFO:             fastapi: Not installed
2024-11-24 16:57:01,107:INFO:             uvicorn: Not installed
2024-11-24 16:57:01,108:INFO:              m2cgen: Not installed
2024-11-24 16:57:01,108:INFO:           evidently: Not installed
2024-11-24 16:57:01,108:INFO:               fugue: Not installed
2024-11-24 16:57:01,109:INFO:           streamlit: Not installed
2024-11-24 16:57:01,110:INFO:             prophet: Not installed
2024-11-24 16:57:01,110:INFO:None
2024-11-24 16:57:01,111:INFO:Set up data.
2024-11-24 16:57:01,127:INFO:Set up folding strategy.
2024-11-24 16:57:01,128:INFO:Set up train/test split.
2024-11-24 16:57:01,140:INFO:Set up index.
2024-11-24 16:57:01,141:INFO:Assigning column types.
2024-11-24 16:57:01,147:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-24 16:57:01,173:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:57:01,174:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:57:01,191:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,192:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,218:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 16:57:01,219:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:57:01,236:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,236:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,237:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-24 16:57:01,263:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:57:01,291:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,292:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,321:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 16:57:01,341:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,342:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,343:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-24 16:57:01,392:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,393:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,451:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,452:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:01,453:INFO:Preparing preprocessing pipeline...
2024-11-24 16:57:01,455:INFO:Set up simple imputation.
2024-11-24 16:57:01,466:INFO:Set up encoding of ordinal features.
2024-11-24 16:57:01,469:INFO:Set up encoding of categorical features.
2024-11-24 16:57:01,814:INFO:Finished creating preprocessing pipeline.
2024-11-24 16:57:01,839:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Quantity', 'Fee',
                                             'VideoAmt', 'PhotoAmt'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=...
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Breed1', 'Breed2', 'RescuerID'],
                                    transformer=TargetEncoder(cols=['Breed1',
                                                                    'Breed2',
                                                                    'RescuerID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-24 16:57:01,839:INFO:Creating final display dataframe.
2024-11-24 16:57:02,032:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target     AdoptionSpeed
2                   Target type        Multiclass
3           Original data shape       (14993, 21)
4        Transformed data shape       (14993, 66)
5   Transformed train set shape       (10495, 66)
6    Transformed test set shape        (4498, 66)
7              Numeric features                 5
8          Categorical features                15
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              9ada
2024-11-24 16:57:02,098:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:02,098:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:02,147:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:02,148:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 16:57:02,149:INFO:setup() successfully completed in 1.11s...............
2024-11-24 16:57:11,801:INFO:Initializing compare_models()
2024-11-24 16:57:11,802:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-24 16:57:11,803:INFO:Checking exceptions
2024-11-24 16:57:11,811:INFO:Preparing display monitor
2024-11-24 16:57:11,838:INFO:Initializing Logistic Regression
2024-11-24 16:57:11,839:INFO:Total runtime is 7.589658101399739e-06 minutes
2024-11-24 16:57:11,843:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:11,844:INFO:Initializing create_model()
2024-11-24 16:57:11,845:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:11,846:INFO:Checking exceptions
2024-11-24 16:57:11,846:INFO:Importing libraries
2024-11-24 16:57:11,847:INFO:Copying training dataset
2024-11-24 16:57:11,854:INFO:Defining folds
2024-11-24 16:57:11,855:INFO:Declaring metric variables
2024-11-24 16:57:11,859:INFO:Importing untrained model
2024-11-24 16:57:11,863:INFO:Logistic Regression Imported successfully
2024-11-24 16:57:11,869:INFO:Starting cross validation
2024-11-24 16:57:11,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:21,118:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,137:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,147:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,183:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,189:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,205:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,214:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,220:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,233:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,236:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,242:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 16:57:21,266:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,280:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:21,302:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,306:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,310:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:21,313:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,314:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,314:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,315:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:21,320:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:21,320:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:21,321:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,325:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,330:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:21,367:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:21,367:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:21,369:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:21,386:INFO:Calculating mean and std
2024-11-24 16:57:21,390:INFO:Creating metrics dataframe
2024-11-24 16:57:21,395:INFO:Uploading results into container
2024-11-24 16:57:21,397:INFO:Uploading model into container now
2024-11-24 16:57:21,399:INFO:_master_model_container: 1
2024-11-24 16:57:21,399:INFO:_display_container: 2
2024-11-24 16:57:21,400:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-24 16:57:21,400:INFO:create_model() successfully completed......................................
2024-11-24 16:57:21,528:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:21,529:INFO:Creating metrics dataframe
2024-11-24 16:57:21,537:INFO:Initializing K Neighbors Classifier
2024-11-24 16:57:21,537:INFO:Total runtime is 0.1616497317949931 minutes
2024-11-24 16:57:21,542:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:21,543:INFO:Initializing create_model()
2024-11-24 16:57:21,544:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:21,544:INFO:Checking exceptions
2024-11-24 16:57:21,544:INFO:Importing libraries
2024-11-24 16:57:21,547:INFO:Copying training dataset
2024-11-24 16:57:21,563:INFO:Defining folds
2024-11-24 16:57:21,574:INFO:Declaring metric variables
2024-11-24 16:57:21,578:INFO:Importing untrained model
2024-11-24 16:57:21,589:INFO:K Neighbors Classifier Imported successfully
2024-11-24 16:57:21,598:INFO:Starting cross validation
2024-11-24 16:57:21,603:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:25,791:INFO:Calculating mean and std
2024-11-24 16:57:25,793:INFO:Creating metrics dataframe
2024-11-24 16:57:25,796:INFO:Uploading results into container
2024-11-24 16:57:25,797:INFO:Uploading model into container now
2024-11-24 16:57:25,798:INFO:_master_model_container: 2
2024-11-24 16:57:25,803:INFO:_display_container: 2
2024-11-24 16:57:25,804:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-24 16:57:25,805:INFO:create_model() successfully completed......................................
2024-11-24 16:57:25,870:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:25,871:INFO:Creating metrics dataframe
2024-11-24 16:57:25,881:INFO:Initializing Naive Bayes
2024-11-24 16:57:25,886:INFO:Total runtime is 0.2341251254081726 minutes
2024-11-24 16:57:25,892:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:25,893:INFO:Initializing create_model()
2024-11-24 16:57:25,893:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:25,894:INFO:Checking exceptions
2024-11-24 16:57:25,894:INFO:Importing libraries
2024-11-24 16:57:25,894:INFO:Copying training dataset
2024-11-24 16:57:25,907:INFO:Defining folds
2024-11-24 16:57:25,907:INFO:Declaring metric variables
2024-11-24 16:57:25,912:INFO:Importing untrained model
2024-11-24 16:57:25,915:INFO:Naive Bayes Imported successfully
2024-11-24 16:57:25,925:INFO:Starting cross validation
2024-11-24 16:57:25,929:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:26,703:INFO:Calculating mean and std
2024-11-24 16:57:26,705:INFO:Creating metrics dataframe
2024-11-24 16:57:26,707:INFO:Uploading results into container
2024-11-24 16:57:26,708:INFO:Uploading model into container now
2024-11-24 16:57:26,709:INFO:_master_model_container: 3
2024-11-24 16:57:26,710:INFO:_display_container: 2
2024-11-24 16:57:26,710:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-24 16:57:26,711:INFO:create_model() successfully completed......................................
2024-11-24 16:57:26,779:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:26,780:INFO:Creating metrics dataframe
2024-11-24 16:57:26,789:INFO:Initializing Decision Tree Classifier
2024-11-24 16:57:26,789:INFO:Total runtime is 0.24918575286865233 minutes
2024-11-24 16:57:26,794:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:26,795:INFO:Initializing create_model()
2024-11-24 16:57:26,795:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:26,796:INFO:Checking exceptions
2024-11-24 16:57:26,796:INFO:Importing libraries
2024-11-24 16:57:26,797:INFO:Copying training dataset
2024-11-24 16:57:26,808:INFO:Defining folds
2024-11-24 16:57:26,827:INFO:Declaring metric variables
2024-11-24 16:57:26,832:INFO:Importing untrained model
2024-11-24 16:57:26,843:INFO:Decision Tree Classifier Imported successfully
2024-11-24 16:57:26,851:INFO:Starting cross validation
2024-11-24 16:57:26,855:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:27,681:INFO:Calculating mean and std
2024-11-24 16:57:27,683:INFO:Creating metrics dataframe
2024-11-24 16:57:27,685:INFO:Uploading results into container
2024-11-24 16:57:27,686:INFO:Uploading model into container now
2024-11-24 16:57:27,687:INFO:_master_model_container: 4
2024-11-24 16:57:27,687:INFO:_display_container: 2
2024-11-24 16:57:27,688:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-24 16:57:27,689:INFO:create_model() successfully completed......................................
2024-11-24 16:57:27,749:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:27,754:INFO:Creating metrics dataframe
2024-11-24 16:57:27,763:INFO:Initializing SVM - Linear Kernel
2024-11-24 16:57:27,765:INFO:Total runtime is 0.26544566949208576 minutes
2024-11-24 16:57:27,769:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:27,771:INFO:Initializing create_model()
2024-11-24 16:57:27,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:27,772:INFO:Checking exceptions
2024-11-24 16:57:27,773:INFO:Importing libraries
2024-11-24 16:57:27,774:INFO:Copying training dataset
2024-11-24 16:57:27,782:INFO:Defining folds
2024-11-24 16:57:27,783:INFO:Declaring metric variables
2024-11-24 16:57:27,790:INFO:Importing untrained model
2024-11-24 16:57:27,795:INFO:SVM - Linear Kernel Imported successfully
2024-11-24 16:57:27,812:INFO:Starting cross validation
2024-11-24 16:57:27,815:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:28,643:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,706:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,725:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,732:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:28,742:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,749:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:28,780:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,780:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,784:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:28,790:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,793:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:28,822:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,823:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,826:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:28,827:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:28,842:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:28,856:INFO:Calculating mean and std
2024-11-24 16:57:28,857:INFO:Creating metrics dataframe
2024-11-24 16:57:28,861:INFO:Uploading results into container
2024-11-24 16:57:28,862:INFO:Uploading model into container now
2024-11-24 16:57:28,863:INFO:_master_model_container: 5
2024-11-24 16:57:28,863:INFO:_display_container: 2
2024-11-24 16:57:28,864:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-24 16:57:28,864:INFO:create_model() successfully completed......................................
2024-11-24 16:57:28,928:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:28,929:INFO:Creating metrics dataframe
2024-11-24 16:57:28,939:INFO:Initializing Ridge Classifier
2024-11-24 16:57:28,975:INFO:Total runtime is 0.28560858170191444 minutes
2024-11-24 16:57:28,981:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:28,982:INFO:Initializing create_model()
2024-11-24 16:57:28,982:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:28,983:INFO:Checking exceptions
2024-11-24 16:57:28,983:INFO:Importing libraries
2024-11-24 16:57:28,985:INFO:Copying training dataset
2024-11-24 16:57:28,997:INFO:Defining folds
2024-11-24 16:57:28,998:INFO:Declaring metric variables
2024-11-24 16:57:29,004:INFO:Importing untrained model
2024-11-24 16:57:29,010:INFO:Ridge Classifier Imported successfully
2024-11-24 16:57:29,017:INFO:Starting cross validation
2024-11-24 16:57:29,027:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:29,663:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,666:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,671:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,674:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,691:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,695:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,705:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,709:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,713:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,718:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,732:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,738:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,797:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,799:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,802:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,803:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,805:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,807:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,808:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:29,812:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:29,825:INFO:Calculating mean and std
2024-11-24 16:57:29,826:INFO:Creating metrics dataframe
2024-11-24 16:57:29,828:INFO:Uploading results into container
2024-11-24 16:57:29,829:INFO:Uploading model into container now
2024-11-24 16:57:29,830:INFO:_master_model_container: 6
2024-11-24 16:57:29,831:INFO:_display_container: 2
2024-11-24 16:57:29,831:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-24 16:57:29,832:INFO:create_model() successfully completed......................................
2024-11-24 16:57:29,895:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:29,896:INFO:Creating metrics dataframe
2024-11-24 16:57:29,906:INFO:Initializing Random Forest Classifier
2024-11-24 16:57:29,907:INFO:Total runtime is 0.3011419018109639 minutes
2024-11-24 16:57:29,912:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:29,913:INFO:Initializing create_model()
2024-11-24 16:57:29,913:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:29,914:INFO:Checking exceptions
2024-11-24 16:57:29,915:INFO:Importing libraries
2024-11-24 16:57:29,915:INFO:Copying training dataset
2024-11-24 16:57:29,924:INFO:Defining folds
2024-11-24 16:57:29,925:INFO:Declaring metric variables
2024-11-24 16:57:29,930:INFO:Importing untrained model
2024-11-24 16:57:29,938:INFO:Random Forest Classifier Imported successfully
2024-11-24 16:57:29,958:INFO:Starting cross validation
2024-11-24 16:57:29,962:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:32,326:INFO:Calculating mean and std
2024-11-24 16:57:32,327:INFO:Creating metrics dataframe
2024-11-24 16:57:32,329:INFO:Uploading results into container
2024-11-24 16:57:32,330:INFO:Uploading model into container now
2024-11-24 16:57:32,331:INFO:_master_model_container: 7
2024-11-24 16:57:32,331:INFO:_display_container: 2
2024-11-24 16:57:32,332:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-24 16:57:32,332:INFO:create_model() successfully completed......................................
2024-11-24 16:57:32,397:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:32,397:INFO:Creating metrics dataframe
2024-11-24 16:57:32,405:INFO:Initializing Quadratic Discriminant Analysis
2024-11-24 16:57:32,406:INFO:Total runtime is 0.34279162486394243 minutes
2024-11-24 16:57:32,410:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:32,410:INFO:Initializing create_model()
2024-11-24 16:57:32,411:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:32,411:INFO:Checking exceptions
2024-11-24 16:57:32,411:INFO:Importing libraries
2024-11-24 16:57:32,411:INFO:Copying training dataset
2024-11-24 16:57:32,419:INFO:Defining folds
2024-11-24 16:57:32,420:INFO:Declaring metric variables
2024-11-24 16:57:32,424:INFO:Importing untrained model
2024-11-24 16:57:32,428:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-24 16:57:32,441:INFO:Starting cross validation
2024-11-24 16:57:32,454:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:33,109:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,182:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,245:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,270:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,274:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,289:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,336:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,337:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,344:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,365:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,369:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,373:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:33,375:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 16:57:33,431:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,436:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,452:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,460:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:33,482:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,494:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,497:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,510:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,513:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:33,517:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 16:57:33,524:INFO:Calculating mean and std
2024-11-24 16:57:33,525:INFO:Creating metrics dataframe
2024-11-24 16:57:33,528:INFO:Uploading results into container
2024-11-24 16:57:33,529:INFO:Uploading model into container now
2024-11-24 16:57:33,530:INFO:_master_model_container: 8
2024-11-24 16:57:33,530:INFO:_display_container: 2
2024-11-24 16:57:33,530:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-24 16:57:33,531:INFO:create_model() successfully completed......................................
2024-11-24 16:57:33,599:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:33,600:INFO:Creating metrics dataframe
2024-11-24 16:57:33,609:INFO:Initializing Ada Boost Classifier
2024-11-24 16:57:33,610:INFO:Total runtime is 0.3628537098566691 minutes
2024-11-24 16:57:33,613:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:33,614:INFO:Initializing create_model()
2024-11-24 16:57:33,614:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:33,614:INFO:Checking exceptions
2024-11-24 16:57:33,615:INFO:Importing libraries
2024-11-24 16:57:33,615:INFO:Copying training dataset
2024-11-24 16:57:33,624:INFO:Defining folds
2024-11-24 16:57:33,625:INFO:Declaring metric variables
2024-11-24 16:57:33,628:INFO:Importing untrained model
2024-11-24 16:57:33,632:INFO:Ada Boost Classifier Imported successfully
2024-11-24 16:57:33,642:INFO:Starting cross validation
2024-11-24 16:57:33,646:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:34,228:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,230:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,242:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,246:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,252:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,260:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,260:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,260:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,263:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,265:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 16:57:34,901:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,903:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,935:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,935:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,935:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,941:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,943:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,943:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,944:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,947:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:34,960:INFO:Calculating mean and std
2024-11-24 16:57:34,961:INFO:Creating metrics dataframe
2024-11-24 16:57:34,964:INFO:Uploading results into container
2024-11-24 16:57:34,965:INFO:Uploading model into container now
2024-11-24 16:57:34,965:INFO:_master_model_container: 9
2024-11-24 16:57:34,965:INFO:_display_container: 2
2024-11-24 16:57:34,966:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-24 16:57:34,966:INFO:create_model() successfully completed......................................
2024-11-24 16:57:35,040:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:35,041:INFO:Creating metrics dataframe
2024-11-24 16:57:35,054:INFO:Initializing Gradient Boosting Classifier
2024-11-24 16:57:35,055:INFO:Total runtime is 0.3869449138641357 minutes
2024-11-24 16:57:35,059:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:35,060:INFO:Initializing create_model()
2024-11-24 16:57:35,061:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:35,061:INFO:Checking exceptions
2024-11-24 16:57:35,062:INFO:Importing libraries
2024-11-24 16:57:35,063:INFO:Copying training dataset
2024-11-24 16:57:35,070:INFO:Defining folds
2024-11-24 16:57:35,071:INFO:Declaring metric variables
2024-11-24 16:57:35,075:INFO:Importing untrained model
2024-11-24 16:57:35,078:INFO:Gradient Boosting Classifier Imported successfully
2024-11-24 16:57:35,087:INFO:Starting cross validation
2024-11-24 16:57:35,091:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:43,434:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,487:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,501:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,510:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,514:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,532:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,561:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,580:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,588:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,623:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:43,635:INFO:Calculating mean and std
2024-11-24 16:57:43,638:INFO:Creating metrics dataframe
2024-11-24 16:57:43,641:INFO:Uploading results into container
2024-11-24 16:57:43,642:INFO:Uploading model into container now
2024-11-24 16:57:43,643:INFO:_master_model_container: 10
2024-11-24 16:57:43,643:INFO:_display_container: 2
2024-11-24 16:57:43,644:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-24 16:57:43,645:INFO:create_model() successfully completed......................................
2024-11-24 16:57:43,719:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:43,726:INFO:Creating metrics dataframe
2024-11-24 16:57:43,746:INFO:Initializing Linear Discriminant Analysis
2024-11-24 16:57:43,755:INFO:Total runtime is 0.5319389343261718 minutes
2024-11-24 16:57:43,760:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:43,761:INFO:Initializing create_model()
2024-11-24 16:57:43,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:43,762:INFO:Checking exceptions
2024-11-24 16:57:43,768:INFO:Importing libraries
2024-11-24 16:57:43,769:INFO:Copying training dataset
2024-11-24 16:57:43,779:INFO:Defining folds
2024-11-24 16:57:43,785:INFO:Declaring metric variables
2024-11-24 16:57:43,791:INFO:Importing untrained model
2024-11-24 16:57:43,795:INFO:Linear Discriminant Analysis Imported successfully
2024-11-24 16:57:43,802:INFO:Starting cross validation
2024-11-24 16:57:43,811:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:44,544:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,573:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,603:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,604:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,607:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,617:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,620:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,630:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,630:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,631:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 16:57:44,656:INFO:Calculating mean and std
2024-11-24 16:57:44,673:INFO:Creating metrics dataframe
2024-11-24 16:57:44,675:INFO:Uploading results into container
2024-11-24 16:57:44,676:INFO:Uploading model into container now
2024-11-24 16:57:44,677:INFO:_master_model_container: 11
2024-11-24 16:57:44,677:INFO:_display_container: 2
2024-11-24 16:57:44,677:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-24 16:57:44,678:INFO:create_model() successfully completed......................................
2024-11-24 16:57:44,750:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:44,751:INFO:Creating metrics dataframe
2024-11-24 16:57:44,762:INFO:Initializing Extra Trees Classifier
2024-11-24 16:57:44,762:INFO:Total runtime is 0.5487347960472106 minutes
2024-11-24 16:57:44,773:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:44,774:INFO:Initializing create_model()
2024-11-24 16:57:44,775:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:44,776:INFO:Checking exceptions
2024-11-24 16:57:44,776:INFO:Importing libraries
2024-11-24 16:57:44,776:INFO:Copying training dataset
2024-11-24 16:57:44,789:INFO:Defining folds
2024-11-24 16:57:44,789:INFO:Declaring metric variables
2024-11-24 16:57:44,794:INFO:Importing untrained model
2024-11-24 16:57:44,800:INFO:Extra Trees Classifier Imported successfully
2024-11-24 16:57:44,808:INFO:Starting cross validation
2024-11-24 16:57:44,812:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 16:57:51,453:INFO:Calculating mean and std
2024-11-24 16:57:51,454:INFO:Creating metrics dataframe
2024-11-24 16:57:51,457:INFO:Uploading results into container
2024-11-24 16:57:51,457:INFO:Uploading model into container now
2024-11-24 16:57:51,458:INFO:_master_model_container: 12
2024-11-24 16:57:51,458:INFO:_display_container: 2
2024-11-24 16:57:51,459:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-24 16:57:51,459:INFO:create_model() successfully completed......................................
2024-11-24 16:57:51,532:INFO:SubProcess create_model() end ==================================
2024-11-24 16:57:51,533:INFO:Creating metrics dataframe
2024-11-24 16:57:51,545:INFO:Initializing Light Gradient Boosting Machine
2024-11-24 16:57:51,545:INFO:Total runtime is 0.6617803494135538 minutes
2024-11-24 16:57:51,550:INFO:SubProcess create_model() called ==================================
2024-11-24 16:57:51,551:INFO:Initializing create_model()
2024-11-24 16:57:51,551:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd40b139de0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd4117c04f0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 16:57:51,552:INFO:Checking exceptions
2024-11-24 16:57:51,552:INFO:Importing libraries
2024-11-24 16:57:51,553:INFO:Copying training dataset
2024-11-24 16:57:51,562:INFO:Defining folds
2024-11-24 16:57:51,564:INFO:Declaring metric variables
2024-11-24 16:57:51,569:INFO:Importing untrained model
2024-11-24 16:57:51,574:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-24 16:57:51,584:INFO:Starting cross validation
2024-11-24 16:57:51,589:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:07:56,244:INFO:PyCaret ClassificationExperiment
2024-11-24 17:07:56,244:INFO:Logging name: clf-default-name
2024-11-24 17:07:56,244:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-24 17:07:56,245:INFO:version 3.3.2
2024-11-24 17:07:56,245:INFO:Initializing setup()
2024-11-24 17:07:56,246:INFO:self.USI: 3c2d
2024-11-24 17:07:56,246:INFO:self._variable_keys: {'gpu_n_jobs_param', 'is_multiclass', 'fix_imbalance', 'fold_groups_param', 'USI', 'data', 'exp_id', '_ml_usecase', 'html_param', 'y_train', 'pipeline', '_available_plots', 'exp_name_log', 'gpu_param', 'fold_generator', 'X_test', 'memory', 'X_train', 'fold_shuffle_param', 'log_plots_param', 'idx', 'y', 'logging_param', 'y_test', 'X', 'seed', 'n_jobs_param', 'target_param'}
2024-11-24 17:07:56,246:INFO:Checking environment
2024-11-24 17:07:56,247:INFO:python_version: 3.10.12
2024-11-24 17:07:56,247:INFO:python_build: ('main', 'Nov  6 2024 20:22:13')
2024-11-24 17:07:56,247:INFO:machine: x86_64
2024-11-24 17:07:56,248:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 17:07:56,248:INFO:Memory: svmem(total=8156200960, available=6120312832, percent=25.0, used=1711923200, free=5402005504, active=287223808, inactive=1687670784, buffers=8843264, cached=1033428992, shared=16826368, slab=452493312)
2024-11-24 17:07:56,250:INFO:Physical Core: 10
2024-11-24 17:07:56,251:INFO:Logical Core: 20
2024-11-24 17:07:56,251:INFO:Checking libraries
2024-11-24 17:07:56,252:INFO:System:
2024-11-24 17:07:56,252:INFO:    python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
2024-11-24 17:07:56,253:INFO:executable: /usr/bin/python3
2024-11-24 17:07:56,253:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 17:07:56,254:INFO:PyCaret required dependencies:
2024-11-24 17:07:56,255:INFO:                 pip: 22.0.2
2024-11-24 17:07:56,255:INFO:          setuptools: 59.6.0
2024-11-24 17:07:56,256:INFO:             pycaret: 3.3.2
2024-11-24 17:07:56,256:INFO:             IPython: 8.29.0
2024-11-24 17:07:56,256:INFO:          ipywidgets: 8.1.5
2024-11-24 17:07:56,257:INFO:                tqdm: 4.67.0
2024-11-24 17:07:56,257:INFO:               numpy: 1.26.4
2024-11-24 17:07:56,257:INFO:              pandas: 2.1.4
2024-11-24 17:07:56,258:INFO:              jinja2: 3.1.4
2024-11-24 17:07:56,258:INFO:               scipy: 1.11.4
2024-11-24 17:07:56,258:INFO:              joblib: 1.3.2
2024-11-24 17:07:56,258:INFO:             sklearn: 1.4.2
2024-11-24 17:07:56,259:INFO:                pyod: 2.0.2
2024-11-24 17:07:56,259:INFO:            imblearn: 0.12.4
2024-11-24 17:07:56,259:INFO:   category_encoders: 2.6.4
2024-11-24 17:07:56,260:INFO:            lightgbm: 4.5.0
2024-11-24 17:07:56,260:INFO:               numba: 0.60.0
2024-11-24 17:07:56,260:INFO:            requests: 2.32.3
2024-11-24 17:07:56,260:INFO:          matplotlib: 3.7.5
2024-11-24 17:07:56,261:INFO:          scikitplot: 0.3.7
2024-11-24 17:07:56,261:INFO:         yellowbrick: 1.5
2024-11-24 17:07:56,261:INFO:              plotly: 5.24.1
2024-11-24 17:07:56,262:INFO:    plotly-resampler: Not installed
2024-11-24 17:07:56,262:INFO:             kaleido: 0.2.1
2024-11-24 17:07:56,262:INFO:           schemdraw: 0.15
2024-11-24 17:07:56,263:INFO:         statsmodels: 0.14.4
2024-11-24 17:07:56,263:INFO:              sktime: 0.26.0
2024-11-24 17:07:56,264:INFO:               tbats: 1.1.3
2024-11-24 17:07:56,264:INFO:            pmdarima: 2.0.4
2024-11-24 17:07:56,265:INFO:              psutil: 6.1.0
2024-11-24 17:07:56,265:INFO:          markupsafe: 3.0.2
2024-11-24 17:07:56,265:INFO:             pickle5: Not installed
2024-11-24 17:07:56,266:INFO:         cloudpickle: 3.1.0
2024-11-24 17:07:56,266:INFO:         deprecation: 2.1.0
2024-11-24 17:07:56,266:INFO:              xxhash: 3.5.0
2024-11-24 17:07:56,267:INFO:           wurlitzer: 3.1.1
2024-11-24 17:07:56,267:INFO:PyCaret optional dependencies:
2024-11-24 17:07:56,268:INFO:                shap: Not installed
2024-11-24 17:07:56,268:INFO:           interpret: Not installed
2024-11-24 17:07:56,269:INFO:                umap: Not installed
2024-11-24 17:07:56,269:INFO:     ydata_profiling: Not installed
2024-11-24 17:07:56,270:INFO:  explainerdashboard: Not installed
2024-11-24 17:07:56,270:INFO:             autoviz: Not installed
2024-11-24 17:07:56,270:INFO:           fairlearn: Not installed
2024-11-24 17:07:56,271:INFO:          deepchecks: Not installed
2024-11-24 17:07:56,271:INFO:             xgboost: Not installed
2024-11-24 17:07:56,271:INFO:            catboost: Not installed
2024-11-24 17:07:56,271:INFO:              kmodes: Not installed
2024-11-24 17:07:56,272:INFO:             mlxtend: Not installed
2024-11-24 17:07:56,272:INFO:       statsforecast: Not installed
2024-11-24 17:07:56,273:INFO:        tune_sklearn: Not installed
2024-11-24 17:07:56,273:INFO:                 ray: Not installed
2024-11-24 17:07:56,273:INFO:            hyperopt: Not installed
2024-11-24 17:07:56,273:INFO:              optuna: Not installed
2024-11-24 17:07:56,274:INFO:               skopt: Not installed
2024-11-24 17:07:56,274:INFO:              mlflow: Not installed
2024-11-24 17:07:56,274:INFO:              gradio: Not installed
2024-11-24 17:07:56,274:INFO:             fastapi: Not installed
2024-11-24 17:07:56,275:INFO:             uvicorn: Not installed
2024-11-24 17:07:56,275:INFO:              m2cgen: Not installed
2024-11-24 17:07:56,275:INFO:           evidently: Not installed
2024-11-24 17:07:56,275:INFO:               fugue: Not installed
2024-11-24 17:07:56,275:INFO:           streamlit: Not installed
2024-11-24 17:07:56,276:INFO:             prophet: Not installed
2024-11-24 17:07:56,276:INFO:None
2024-11-24 17:07:56,276:INFO:Set up data.
2024-11-24 17:07:56,293:INFO:Set up folding strategy.
2024-11-24 17:07:56,294:INFO:Set up train/test split.
2024-11-24 17:07:56,307:INFO:Set up index.
2024-11-24 17:07:56,309:INFO:Assigning column types.
2024-11-24 17:07:56,314:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-24 17:07:56,360:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 17:07:56,362:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 17:07:56,392:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,394:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,423:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 17:07:56,424:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 17:07:56,442:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,443:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,444:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-24 17:07:56,482:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 17:07:56,499:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,500:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,528:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 17:07:56,547:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,548:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,548:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-24 17:07:56,600:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,601:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,647:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,647:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:56,649:INFO:Preparing preprocessing pipeline...
2024-11-24 17:07:56,651:INFO:Set up simple imputation.
2024-11-24 17:07:56,656:INFO:Set up encoding of ordinal features.
2024-11-24 17:07:56,658:INFO:Set up encoding of categorical features.
2024-11-24 17:07:57,186:INFO:Finished creating preprocessing pipeline.
2024-11-24 17:07:57,209:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Quantity', 'Fee',
                                             'VideoAmt', 'PhotoAmt'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=...
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Breed1', 'Breed2', 'RescuerID'],
                                    transformer=TargetEncoder(cols=['Breed1',
                                                                    'Breed2',
                                                                    'RescuerID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-24 17:07:57,210:INFO:Creating final display dataframe.
2024-11-24 17:07:57,935:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target     AdoptionSpeed
2                   Target type        Multiclass
3           Original data shape       (13494, 21)
4        Transformed data shape       (13494, 66)
5   Transformed train set shape        (9445, 66)
6    Transformed test set shape        (4049, 66)
7              Numeric features                 5
8          Categorical features                15
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              3c2d
2024-11-24 17:07:58,017:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:58,018:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:58,067:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:58,068:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:07:58,069:INFO:setup() successfully completed in 1.83s...............
2024-11-24 17:08:09,365:INFO:Initializing compare_models()
2024-11-24 17:08:09,365:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-24 17:08:09,366:INFO:Checking exceptions
2024-11-24 17:08:09,372:INFO:Preparing display monitor
2024-11-24 17:08:09,399:INFO:Initializing Logistic Regression
2024-11-24 17:08:09,399:INFO:Total runtime is 1.031955083211263e-05 minutes
2024-11-24 17:08:09,403:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:09,404:INFO:Initializing create_model()
2024-11-24 17:08:09,404:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:09,405:INFO:Checking exceptions
2024-11-24 17:08:09,405:INFO:Importing libraries
2024-11-24 17:08:09,406:INFO:Copying training dataset
2024-11-24 17:08:09,413:INFO:Defining folds
2024-11-24 17:08:09,414:INFO:Declaring metric variables
2024-11-24 17:08:09,418:INFO:Importing untrained model
2024-11-24 17:08:09,422:INFO:Logistic Regression Imported successfully
2024-11-24 17:08:09,429:INFO:Starting cross validation
2024-11-24 17:08:09,432:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:16,524:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,562:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,574:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,578:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,580:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,584:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,603:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,607:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,629:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,649:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,656:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:16,682:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,689:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,693:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:16,700:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,710:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,712:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:08:16,721:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,722:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:16,723:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,730:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,731:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:16,733:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:16,738:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:16,758:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,819:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:16,824:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:16,840:INFO:Calculating mean and std
2024-11-24 17:08:16,842:INFO:Creating metrics dataframe
2024-11-24 17:08:16,847:INFO:Uploading results into container
2024-11-24 17:08:16,849:INFO:Uploading model into container now
2024-11-24 17:08:16,850:INFO:_master_model_container: 1
2024-11-24 17:08:16,851:INFO:_display_container: 2
2024-11-24 17:08:16,851:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-24 17:08:16,852:INFO:create_model() successfully completed......................................
2024-11-24 17:08:16,985:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:16,986:INFO:Creating metrics dataframe
2024-11-24 17:08:16,995:INFO:Initializing K Neighbors Classifier
2024-11-24 17:08:16,996:INFO:Total runtime is 0.12662384112675984 minutes
2024-11-24 17:08:17,001:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:17,002:INFO:Initializing create_model()
2024-11-24 17:08:17,002:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:17,003:INFO:Checking exceptions
2024-11-24 17:08:17,003:INFO:Importing libraries
2024-11-24 17:08:17,004:INFO:Copying training dataset
2024-11-24 17:08:17,014:INFO:Defining folds
2024-11-24 17:08:17,015:INFO:Declaring metric variables
2024-11-24 17:08:17,020:INFO:Importing untrained model
2024-11-24 17:08:17,026:INFO:K Neighbors Classifier Imported successfully
2024-11-24 17:08:17,040:INFO:Starting cross validation
2024-11-24 17:08:17,049:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:20,282:INFO:Calculating mean and std
2024-11-24 17:08:20,286:INFO:Creating metrics dataframe
2024-11-24 17:08:20,290:INFO:Uploading results into container
2024-11-24 17:08:20,292:INFO:Uploading model into container now
2024-11-24 17:08:20,293:INFO:_master_model_container: 2
2024-11-24 17:08:20,293:INFO:_display_container: 2
2024-11-24 17:08:20,294:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-24 17:08:20,295:INFO:create_model() successfully completed......................................
2024-11-24 17:08:20,396:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:20,400:INFO:Creating metrics dataframe
2024-11-24 17:08:20,408:INFO:Initializing Naive Bayes
2024-11-24 17:08:20,408:INFO:Total runtime is 0.18348997433980305 minutes
2024-11-24 17:08:20,412:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:20,417:INFO:Initializing create_model()
2024-11-24 17:08:20,419:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:20,419:INFO:Checking exceptions
2024-11-24 17:08:20,420:INFO:Importing libraries
2024-11-24 17:08:20,420:INFO:Copying training dataset
2024-11-24 17:08:20,428:INFO:Defining folds
2024-11-24 17:08:20,428:INFO:Declaring metric variables
2024-11-24 17:08:20,433:INFO:Importing untrained model
2024-11-24 17:08:20,438:INFO:Naive Bayes Imported successfully
2024-11-24 17:08:20,445:INFO:Starting cross validation
2024-11-24 17:08:20,448:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:21,369:INFO:Calculating mean and std
2024-11-24 17:08:21,371:INFO:Creating metrics dataframe
2024-11-24 17:08:21,374:INFO:Uploading results into container
2024-11-24 17:08:21,376:INFO:Uploading model into container now
2024-11-24 17:08:21,377:INFO:_master_model_container: 3
2024-11-24 17:08:21,377:INFO:_display_container: 2
2024-11-24 17:08:21,378:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-24 17:08:21,379:INFO:create_model() successfully completed......................................
2024-11-24 17:08:21,478:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:21,478:INFO:Creating metrics dataframe
2024-11-24 17:08:21,487:INFO:Initializing Decision Tree Classifier
2024-11-24 17:08:21,488:INFO:Total runtime is 0.20148687362670897 minutes
2024-11-24 17:08:21,493:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:21,494:INFO:Initializing create_model()
2024-11-24 17:08:21,494:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:21,495:INFO:Checking exceptions
2024-11-24 17:08:21,495:INFO:Importing libraries
2024-11-24 17:08:21,495:INFO:Copying training dataset
2024-11-24 17:08:21,507:INFO:Defining folds
2024-11-24 17:08:21,508:INFO:Declaring metric variables
2024-11-24 17:08:21,513:INFO:Importing untrained model
2024-11-24 17:08:21,519:INFO:Decision Tree Classifier Imported successfully
2024-11-24 17:08:21,529:INFO:Starting cross validation
2024-11-24 17:08:21,534:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:22,382:INFO:Calculating mean and std
2024-11-24 17:08:22,384:INFO:Creating metrics dataframe
2024-11-24 17:08:22,387:INFO:Uploading results into container
2024-11-24 17:08:22,388:INFO:Uploading model into container now
2024-11-24 17:08:22,389:INFO:_master_model_container: 4
2024-11-24 17:08:22,389:INFO:_display_container: 2
2024-11-24 17:08:22,390:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-24 17:08:22,390:INFO:create_model() successfully completed......................................
2024-11-24 17:08:22,488:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:22,488:INFO:Creating metrics dataframe
2024-11-24 17:08:22,496:INFO:Initializing SVM - Linear Kernel
2024-11-24 17:08:22,497:INFO:Total runtime is 0.21830721298853556 minutes
2024-11-24 17:08:22,506:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:22,507:INFO:Initializing create_model()
2024-11-24 17:08:22,507:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:22,508:INFO:Checking exceptions
2024-11-24 17:08:22,508:INFO:Importing libraries
2024-11-24 17:08:22,509:INFO:Copying training dataset
2024-11-24 17:08:22,519:INFO:Defining folds
2024-11-24 17:08:22,520:INFO:Declaring metric variables
2024-11-24 17:08:22,527:INFO:Importing untrained model
2024-11-24 17:08:22,532:INFO:SVM - Linear Kernel Imported successfully
2024-11-24 17:08:22,542:INFO:Starting cross validation
2024-11-24 17:08:22,545:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:23,528:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,542:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,563:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,573:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:23,584:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,592:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:23,623:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,626:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,655:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,668:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,669:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,673:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:23,673:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:23,677:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:23,693:INFO:Calculating mean and std
2024-11-24 17:08:23,695:INFO:Creating metrics dataframe
2024-11-24 17:08:23,698:INFO:Uploading results into container
2024-11-24 17:08:23,702:INFO:Uploading model into container now
2024-11-24 17:08:23,703:INFO:_master_model_container: 5
2024-11-24 17:08:23,704:INFO:_display_container: 2
2024-11-24 17:08:23,704:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-24 17:08:23,705:INFO:create_model() successfully completed......................................
2024-11-24 17:08:23,798:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:23,799:INFO:Creating metrics dataframe
2024-11-24 17:08:23,809:INFO:Initializing Ridge Classifier
2024-11-24 17:08:23,809:INFO:Total runtime is 0.24017632007598877 minutes
2024-11-24 17:08:23,814:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:23,815:INFO:Initializing create_model()
2024-11-24 17:08:23,815:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:23,816:INFO:Checking exceptions
2024-11-24 17:08:23,816:INFO:Importing libraries
2024-11-24 17:08:23,817:INFO:Copying training dataset
2024-11-24 17:08:23,828:INFO:Defining folds
2024-11-24 17:08:23,829:INFO:Declaring metric variables
2024-11-24 17:08:23,833:INFO:Importing untrained model
2024-11-24 17:08:23,838:INFO:Ridge Classifier Imported successfully
2024-11-24 17:08:23,846:INFO:Starting cross validation
2024-11-24 17:08:23,850:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:24,508:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,516:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,528:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,535:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,537:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,537:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,544:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,545:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,545:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,551:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,558:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,559:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,559:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,559:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,563:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,564:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,564:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,565:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,569:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:24,581:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:24,590:INFO:Calculating mean and std
2024-11-24 17:08:24,591:INFO:Creating metrics dataframe
2024-11-24 17:08:24,594:INFO:Uploading results into container
2024-11-24 17:08:24,595:INFO:Uploading model into container now
2024-11-24 17:08:24,595:INFO:_master_model_container: 6
2024-11-24 17:08:24,595:INFO:_display_container: 2
2024-11-24 17:08:24,596:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-24 17:08:24,596:INFO:create_model() successfully completed......................................
2024-11-24 17:08:24,696:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:24,697:INFO:Creating metrics dataframe
2024-11-24 17:08:24,708:INFO:Initializing Random Forest Classifier
2024-11-24 17:08:24,708:INFO:Total runtime is 0.2551602880160014 minutes
2024-11-24 17:08:24,714:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:24,714:INFO:Initializing create_model()
2024-11-24 17:08:24,715:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:24,715:INFO:Checking exceptions
2024-11-24 17:08:24,716:INFO:Importing libraries
2024-11-24 17:08:24,719:INFO:Copying training dataset
2024-11-24 17:08:24,729:INFO:Defining folds
2024-11-24 17:08:24,729:INFO:Declaring metric variables
2024-11-24 17:08:24,736:INFO:Importing untrained model
2024-11-24 17:08:24,741:INFO:Random Forest Classifier Imported successfully
2024-11-24 17:08:24,748:INFO:Starting cross validation
2024-11-24 17:08:24,753:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:27,082:INFO:Calculating mean and std
2024-11-24 17:08:27,083:INFO:Creating metrics dataframe
2024-11-24 17:08:27,086:INFO:Uploading results into container
2024-11-24 17:08:27,087:INFO:Uploading model into container now
2024-11-24 17:08:27,088:INFO:_master_model_container: 7
2024-11-24 17:08:27,088:INFO:_display_container: 2
2024-11-24 17:08:27,089:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-24 17:08:27,089:INFO:create_model() successfully completed......................................
2024-11-24 17:08:27,186:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:27,190:INFO:Creating metrics dataframe
2024-11-24 17:08:27,201:INFO:Initializing Quadratic Discriminant Analysis
2024-11-24 17:08:27,208:INFO:Total runtime is 0.29682125250498453 minutes
2024-11-24 17:08:27,213:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:27,215:INFO:Initializing create_model()
2024-11-24 17:08:27,215:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:27,216:INFO:Checking exceptions
2024-11-24 17:08:27,217:INFO:Importing libraries
2024-11-24 17:08:27,217:INFO:Copying training dataset
2024-11-24 17:08:27,229:INFO:Defining folds
2024-11-24 17:08:27,229:INFO:Declaring metric variables
2024-11-24 17:08:27,235:INFO:Importing untrained model
2024-11-24 17:08:27,245:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-24 17:08:27,252:INFO:Starting cross validation
2024-11-24 17:08:27,261:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:27,767:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,784:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,787:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,806:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,814:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,827:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,860:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,914:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,919:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,919:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:08:27,924:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:27,931:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:27,941:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:27,950:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:27,960:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:27,960:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:27,964:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:08:27,982:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:28,001:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:28,027:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:28,034:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:28,037:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:28,053:INFO:Calculating mean and std
2024-11-24 17:08:28,055:INFO:Creating metrics dataframe
2024-11-24 17:08:28,057:INFO:Uploading results into container
2024-11-24 17:08:28,058:INFO:Uploading model into container now
2024-11-24 17:08:28,059:INFO:_master_model_container: 8
2024-11-24 17:08:28,060:INFO:_display_container: 2
2024-11-24 17:08:28,061:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-24 17:08:28,061:INFO:create_model() successfully completed......................................
2024-11-24 17:08:28,153:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:28,153:INFO:Creating metrics dataframe
2024-11-24 17:08:28,167:INFO:Initializing Ada Boost Classifier
2024-11-24 17:08:28,167:INFO:Total runtime is 0.31280319690704345 minutes
2024-11-24 17:08:28,173:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:28,173:INFO:Initializing create_model()
2024-11-24 17:08:28,174:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:28,174:INFO:Checking exceptions
2024-11-24 17:08:28,175:INFO:Importing libraries
2024-11-24 17:08:28,175:INFO:Copying training dataset
2024-11-24 17:08:28,189:INFO:Defining folds
2024-11-24 17:08:28,190:INFO:Declaring metric variables
2024-11-24 17:08:28,196:INFO:Importing untrained model
2024-11-24 17:08:28,202:INFO:Ada Boost Classifier Imported successfully
2024-11-24 17:08:28,213:INFO:Starting cross validation
2024-11-24 17:08:28,217:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:28,688:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:28,717:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:28,759:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:28,765:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:28,795:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:28,810:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:28,812:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:28,843:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:28,843:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:28,854:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:08:29,447:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,474:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,498:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,510:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,536:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,550:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,561:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,565:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,572:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,582:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:29,595:INFO:Calculating mean and std
2024-11-24 17:08:29,597:INFO:Creating metrics dataframe
2024-11-24 17:08:29,599:INFO:Uploading results into container
2024-11-24 17:08:29,600:INFO:Uploading model into container now
2024-11-24 17:08:29,601:INFO:_master_model_container: 9
2024-11-24 17:08:29,601:INFO:_display_container: 2
2024-11-24 17:08:29,601:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-24 17:08:29,602:INFO:create_model() successfully completed......................................
2024-11-24 17:08:29,695:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:29,696:INFO:Creating metrics dataframe
2024-11-24 17:08:29,708:INFO:Initializing Gradient Boosting Classifier
2024-11-24 17:08:29,709:INFO:Total runtime is 0.338502562046051 minutes
2024-11-24 17:08:29,713:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:29,714:INFO:Initializing create_model()
2024-11-24 17:08:29,715:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:29,715:INFO:Checking exceptions
2024-11-24 17:08:29,716:INFO:Importing libraries
2024-11-24 17:08:29,716:INFO:Copying training dataset
2024-11-24 17:08:29,724:INFO:Defining folds
2024-11-24 17:08:29,725:INFO:Declaring metric variables
2024-11-24 17:08:29,729:INFO:Importing untrained model
2024-11-24 17:08:29,734:INFO:Gradient Boosting Classifier Imported successfully
2024-11-24 17:08:29,743:INFO:Starting cross validation
2024-11-24 17:08:29,747:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:38,520:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,528:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,538:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,547:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,577:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,640:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,647:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,661:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,683:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,705:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:38,717:INFO:Calculating mean and std
2024-11-24 17:08:38,719:INFO:Creating metrics dataframe
2024-11-24 17:08:38,721:INFO:Uploading results into container
2024-11-24 17:08:38,723:INFO:Uploading model into container now
2024-11-24 17:08:38,724:INFO:_master_model_container: 10
2024-11-24 17:08:38,724:INFO:_display_container: 2
2024-11-24 17:08:38,725:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-24 17:08:38,725:INFO:create_model() successfully completed......................................
2024-11-24 17:08:38,819:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:38,837:INFO:Creating metrics dataframe
2024-11-24 17:08:38,847:INFO:Initializing Linear Discriminant Analysis
2024-11-24 17:08:38,847:INFO:Total runtime is 0.4908079902331034 minutes
2024-11-24 17:08:38,852:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:38,852:INFO:Initializing create_model()
2024-11-24 17:08:38,853:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:38,853:INFO:Checking exceptions
2024-11-24 17:08:38,853:INFO:Importing libraries
2024-11-24 17:08:38,854:INFO:Copying training dataset
2024-11-24 17:08:38,865:INFO:Defining folds
2024-11-24 17:08:38,865:INFO:Declaring metric variables
2024-11-24 17:08:38,870:INFO:Importing untrained model
2024-11-24 17:08:38,874:INFO:Linear Discriminant Analysis Imported successfully
2024-11-24 17:08:38,882:INFO:Starting cross validation
2024-11-24 17:08:38,888:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:39,674:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,704:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,720:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,727:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,732:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,734:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,741:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,742:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,757:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,759:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:08:39,778:INFO:Calculating mean and std
2024-11-24 17:08:39,780:INFO:Creating metrics dataframe
2024-11-24 17:08:39,782:INFO:Uploading results into container
2024-11-24 17:08:39,783:INFO:Uploading model into container now
2024-11-24 17:08:39,784:INFO:_master_model_container: 11
2024-11-24 17:08:39,784:INFO:_display_container: 2
2024-11-24 17:08:39,785:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-24 17:08:39,785:INFO:create_model() successfully completed......................................
2024-11-24 17:08:39,886:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:39,887:INFO:Creating metrics dataframe
2024-11-24 17:08:39,899:INFO:Initializing Extra Trees Classifier
2024-11-24 17:08:39,900:INFO:Total runtime is 0.5083455562591552 minutes
2024-11-24 17:08:39,903:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:39,904:INFO:Initializing create_model()
2024-11-24 17:08:39,905:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:39,905:INFO:Checking exceptions
2024-11-24 17:08:39,906:INFO:Importing libraries
2024-11-24 17:08:39,906:INFO:Copying training dataset
2024-11-24 17:08:39,917:INFO:Defining folds
2024-11-24 17:08:39,918:INFO:Declaring metric variables
2024-11-24 17:08:39,922:INFO:Importing untrained model
2024-11-24 17:08:39,926:INFO:Extra Trees Classifier Imported successfully
2024-11-24 17:08:39,936:INFO:Starting cross validation
2024-11-24 17:08:39,940:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:08:43,993:INFO:Calculating mean and std
2024-11-24 17:08:43,996:INFO:Creating metrics dataframe
2024-11-24 17:08:44,001:INFO:Uploading results into container
2024-11-24 17:08:44,003:INFO:Uploading model into container now
2024-11-24 17:08:44,004:INFO:_master_model_container: 12
2024-11-24 17:08:44,004:INFO:_display_container: 2
2024-11-24 17:08:44,005:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-24 17:08:44,005:INFO:create_model() successfully completed......................................
2024-11-24 17:08:44,125:INFO:SubProcess create_model() end ==================================
2024-11-24 17:08:44,125:INFO:Creating metrics dataframe
2024-11-24 17:08:44,139:INFO:Initializing Light Gradient Boosting Machine
2024-11-24 17:08:44,140:INFO:Total runtime is 0.579018751780192 minutes
2024-11-24 17:08:44,146:INFO:SubProcess create_model() called ==================================
2024-11-24 17:08:44,147:INFO:Initializing create_model()
2024-11-24 17:08:44,147:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd47fdf3460>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd40b224910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:08:44,148:INFO:Checking exceptions
2024-11-24 17:08:44,148:INFO:Importing libraries
2024-11-24 17:08:44,148:INFO:Copying training dataset
2024-11-24 17:08:44,162:INFO:Defining folds
2024-11-24 17:08:44,163:INFO:Declaring metric variables
2024-11-24 17:08:44,171:INFO:Importing untrained model
2024-11-24 17:08:44,178:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-24 17:08:44,191:INFO:Starting cross validation
2024-11-24 17:08:44,197:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:45:57,333:INFO:PyCaret ClassificationExperiment
2024-11-24 17:45:57,334:INFO:Logging name: clf-default-name
2024-11-24 17:45:57,335:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-24 17:45:57,336:INFO:version 3.3.2
2024-11-24 17:45:57,337:INFO:Initializing setup()
2024-11-24 17:45:57,338:INFO:self.USI: f952
2024-11-24 17:45:57,339:INFO:self._variable_keys: {'is_multiclass', 'target_param', '_ml_usecase', '_available_plots', 'fold_shuffle_param', 'fold_generator', 'USI', 'idx', 'exp_id', 'y_test', 'fix_imbalance', 'seed', 'y', 'y_train', 'logging_param', 'gpu_n_jobs_param', 'exp_name_log', 'X_test', 'pipeline', 'data', 'memory', 'html_param', 'log_plots_param', 'gpu_param', 'X_train', 'n_jobs_param', 'X', 'fold_groups_param'}
2024-11-24 17:45:57,340:INFO:Checking environment
2024-11-24 17:45:57,340:INFO:python_version: 3.10.12
2024-11-24 17:45:57,341:INFO:python_build: ('main', 'Nov  6 2024 20:22:13')
2024-11-24 17:45:57,342:INFO:machine: x86_64
2024-11-24 17:45:57,343:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 17:45:57,344:INFO:Memory: svmem(total=8156200960, available=6241677312, percent=23.5, used=1590177792, free=5507362816, active=315535360, inactive=1562230784, buffers=10645504, cached=1048014848, shared=16695296, slab=452595712)
2024-11-24 17:45:57,346:INFO:Physical Core: 10
2024-11-24 17:45:57,347:INFO:Logical Core: 20
2024-11-24 17:45:57,348:INFO:Checking libraries
2024-11-24 17:45:57,349:INFO:System:
2024-11-24 17:45:57,350:INFO:    python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
2024-11-24 17:45:57,350:INFO:executable: /usr/bin/python3
2024-11-24 17:45:57,351:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-11-24 17:45:57,352:INFO:PyCaret required dependencies:
2024-11-24 17:45:57,450:INFO:                 pip: 22.0.2
2024-11-24 17:45:57,451:INFO:          setuptools: 59.6.0
2024-11-24 17:45:57,452:INFO:             pycaret: 3.3.2
2024-11-24 17:45:57,453:INFO:             IPython: 8.29.0
2024-11-24 17:45:57,454:INFO:          ipywidgets: 8.1.5
2024-11-24 17:45:57,456:INFO:                tqdm: 4.67.0
2024-11-24 17:45:57,457:INFO:               numpy: 1.26.4
2024-11-24 17:45:57,458:INFO:              pandas: 2.1.4
2024-11-24 17:45:57,459:INFO:              jinja2: 3.1.4
2024-11-24 17:45:57,460:INFO:               scipy: 1.11.4
2024-11-24 17:45:57,461:INFO:              joblib: 1.3.2
2024-11-24 17:45:57,461:INFO:             sklearn: 1.4.2
2024-11-24 17:45:57,462:INFO:                pyod: 2.0.2
2024-11-24 17:45:57,463:INFO:            imblearn: 0.12.4
2024-11-24 17:45:57,464:INFO:   category_encoders: 2.6.4
2024-11-24 17:45:57,464:INFO:            lightgbm: 4.5.0
2024-11-24 17:45:57,465:INFO:               numba: 0.60.0
2024-11-24 17:45:57,466:INFO:            requests: 2.32.3
2024-11-24 17:45:57,467:INFO:          matplotlib: 3.7.5
2024-11-24 17:45:57,467:INFO:          scikitplot: 0.3.7
2024-11-24 17:45:57,468:INFO:         yellowbrick: 1.5
2024-11-24 17:45:57,469:INFO:              plotly: 5.24.1
2024-11-24 17:45:57,470:INFO:    plotly-resampler: Not installed
2024-11-24 17:45:57,471:INFO:             kaleido: 0.2.1
2024-11-24 17:45:57,472:INFO:           schemdraw: 0.15
2024-11-24 17:45:57,472:INFO:         statsmodels: 0.14.4
2024-11-24 17:45:57,473:INFO:              sktime: 0.26.0
2024-11-24 17:45:57,474:INFO:               tbats: 1.1.3
2024-11-24 17:45:57,475:INFO:            pmdarima: 2.0.4
2024-11-24 17:45:57,476:INFO:              psutil: 6.1.0
2024-11-24 17:45:57,477:INFO:          markupsafe: 3.0.2
2024-11-24 17:45:57,478:INFO:             pickle5: Not installed
2024-11-24 17:45:57,479:INFO:         cloudpickle: 3.1.0
2024-11-24 17:45:57,479:INFO:         deprecation: 2.1.0
2024-11-24 17:45:57,480:INFO:              xxhash: 3.5.0
2024-11-24 17:45:57,481:INFO:           wurlitzer: 3.1.1
2024-11-24 17:45:57,482:INFO:PyCaret optional dependencies:
2024-11-24 17:45:57,574:INFO:                shap: Not installed
2024-11-24 17:45:57,575:INFO:           interpret: Not installed
2024-11-24 17:45:57,576:INFO:                umap: Not installed
2024-11-24 17:45:57,577:INFO:     ydata_profiling: Not installed
2024-11-24 17:45:57,578:INFO:  explainerdashboard: Not installed
2024-11-24 17:45:57,579:INFO:             autoviz: Not installed
2024-11-24 17:45:57,579:INFO:           fairlearn: Not installed
2024-11-24 17:45:57,580:INFO:          deepchecks: Not installed
2024-11-24 17:45:57,581:INFO:             xgboost: Not installed
2024-11-24 17:45:57,582:INFO:            catboost: Not installed
2024-11-24 17:45:57,582:INFO:              kmodes: Not installed
2024-11-24 17:45:57,583:INFO:             mlxtend: Not installed
2024-11-24 17:45:57,584:INFO:       statsforecast: Not installed
2024-11-24 17:45:57,585:INFO:        tune_sklearn: Not installed
2024-11-24 17:45:57,585:INFO:                 ray: Not installed
2024-11-24 17:45:57,586:INFO:            hyperopt: Not installed
2024-11-24 17:45:57,587:INFO:              optuna: Not installed
2024-11-24 17:45:57,588:INFO:               skopt: Not installed
2024-11-24 17:45:57,589:INFO:              mlflow: Not installed
2024-11-24 17:45:57,590:INFO:              gradio: Not installed
2024-11-24 17:45:57,590:INFO:             fastapi: Not installed
2024-11-24 17:45:57,591:INFO:             uvicorn: Not installed
2024-11-24 17:45:57,591:INFO:              m2cgen: Not installed
2024-11-24 17:45:57,592:INFO:           evidently: Not installed
2024-11-24 17:45:57,593:INFO:               fugue: Not installed
2024-11-24 17:45:57,595:INFO:           streamlit: Not installed
2024-11-24 17:45:57,595:INFO:             prophet: Not installed
2024-11-24 17:45:57,596:INFO:None
2024-11-24 17:45:57,597:INFO:Set up data.
2024-11-24 17:45:57,623:INFO:Set up folding strategy.
2024-11-24 17:45:57,625:INFO:Set up train/test split.
2024-11-24 17:45:57,658:INFO:Set up index.
2024-11-24 17:45:57,660:INFO:Assigning column types.
2024-11-24 17:45:57,672:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-24 17:45:57,721:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 17:45:57,728:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 17:45:57,770:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:57,771:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:57,816:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-24 17:45:57,818:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 17:45:57,861:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:57,862:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:57,863:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-24 17:45:57,914:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 17:45:57,953:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:57,954:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:58,002:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-24 17:45:58,038:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:58,040:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:58,041:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-24 17:45:58,120:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:58,121:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:58,193:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:58,195:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:45:58,199:INFO:Preparing preprocessing pipeline...
2024-11-24 17:45:58,203:INFO:Set up simple imputation.
2024-11-24 17:45:58,213:INFO:Set up encoding of ordinal features.
2024-11-24 17:45:58,217:INFO:Set up encoding of categorical features.
2024-11-24 17:45:59,040:INFO:Finished creating preprocessing pipeline.
2024-11-24 17:45:59,078:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Quantity', 'Fee',
                                             'VideoAmt', 'PhotoAmt'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=...
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Breed1', 'Breed2', 'RescuerID'],
                                    transformer=TargetEncoder(cols=['Breed1',
                                                                    'Breed2',
                                                                    'RescuerID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-24 17:45:59,079:INFO:Creating final display dataframe.
2024-11-24 17:46:00,304:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target     AdoptionSpeed
2                   Target type        Multiclass
3           Original data shape       (13494, 21)
4        Transformed data shape       (13494, 66)
5   Transformed train set shape        (9445, 66)
6    Transformed test set shape        (4049, 66)
7              Numeric features                 5
8          Categorical features                15
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              f952
2024-11-24 17:46:00,410:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:46:00,411:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:46:00,481:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:46:00,482:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-24 17:46:00,484:INFO:setup() successfully completed in 3.17s...............
2024-11-24 17:46:04,271:INFO:Initializing compare_models()
2024-11-24 17:46:04,272:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-24 17:46:04,273:INFO:Checking exceptions
2024-11-24 17:46:04,282:INFO:Preparing display monitor
2024-11-24 17:46:04,333:INFO:Initializing Logistic Regression
2024-11-24 17:46:04,334:INFO:Total runtime is 1.3848145802815756e-05 minutes
2024-11-24 17:46:04,340:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:04,341:INFO:Initializing create_model()
2024-11-24 17:46:04,342:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:04,342:INFO:Checking exceptions
2024-11-24 17:46:04,343:INFO:Importing libraries
2024-11-24 17:46:04,344:INFO:Copying training dataset
2024-11-24 17:46:04,354:INFO:Defining folds
2024-11-24 17:46:04,355:INFO:Declaring metric variables
2024-11-24 17:46:04,360:INFO:Importing untrained model
2024-11-24 17:46:04,365:INFO:Logistic Regression Imported successfully
2024-11-24 17:46:04,375:INFO:Starting cross validation
2024-11-24 17:46:04,381:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:13,594:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,606:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,618:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,626:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,653:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,663:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,681:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,735:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,741:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,743:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,747:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,751:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,753:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:13,757:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:13,758:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 17:46:13,761:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:13,762:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,771:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:13,786:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,796:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:13,800:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,812:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:13,816:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,864:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,876:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,883:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:13,893:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:13,909:INFO:Calculating mean and std
2024-11-24 17:46:13,914:INFO:Creating metrics dataframe
2024-11-24 17:46:13,923:INFO:Uploading results into container
2024-11-24 17:46:13,926:INFO:Uploading model into container now
2024-11-24 17:46:13,928:INFO:_master_model_container: 1
2024-11-24 17:46:13,929:INFO:_display_container: 2
2024-11-24 17:46:13,931:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-24 17:46:13,931:INFO:create_model() successfully completed......................................
2024-11-24 17:46:14,093:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:14,094:INFO:Creating metrics dataframe
2024-11-24 17:46:14,110:INFO:Initializing K Neighbors Classifier
2024-11-24 17:46:14,111:INFO:Total runtime is 0.1629680355389913 minutes
2024-11-24 17:46:14,119:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:14,121:INFO:Initializing create_model()
2024-11-24 17:46:14,122:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:14,123:INFO:Checking exceptions
2024-11-24 17:46:14,123:INFO:Importing libraries
2024-11-24 17:46:14,124:INFO:Copying training dataset
2024-11-24 17:46:14,143:INFO:Defining folds
2024-11-24 17:46:14,145:INFO:Declaring metric variables
2024-11-24 17:46:14,155:INFO:Importing untrained model
2024-11-24 17:46:14,163:INFO:K Neighbors Classifier Imported successfully
2024-11-24 17:46:14,183:INFO:Starting cross validation
2024-11-24 17:46:14,189:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:19,000:INFO:Calculating mean and std
2024-11-24 17:46:19,088:INFO:Creating metrics dataframe
2024-11-24 17:46:19,133:INFO:Uploading results into container
2024-11-24 17:46:19,145:INFO:Uploading model into container now
2024-11-24 17:46:19,155:INFO:_master_model_container: 2
2024-11-24 17:46:19,162:INFO:_display_container: 2
2024-11-24 17:46:19,168:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-24 17:46:19,173:INFO:create_model() successfully completed......................................
2024-11-24 17:46:19,338:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:19,339:INFO:Creating metrics dataframe
2024-11-24 17:46:19,351:INFO:Initializing Naive Bayes
2024-11-24 17:46:19,352:INFO:Total runtime is 0.2503112157185873 minutes
2024-11-24 17:46:19,359:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:19,360:INFO:Initializing create_model()
2024-11-24 17:46:19,362:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:19,362:INFO:Checking exceptions
2024-11-24 17:46:19,363:INFO:Importing libraries
2024-11-24 17:46:19,363:INFO:Copying training dataset
2024-11-24 17:46:19,377:INFO:Defining folds
2024-11-24 17:46:19,378:INFO:Declaring metric variables
2024-11-24 17:46:19,384:INFO:Importing untrained model
2024-11-24 17:46:19,390:INFO:Naive Bayes Imported successfully
2024-11-24 17:46:19,400:INFO:Starting cross validation
2024-11-24 17:46:19,405:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:20,406:INFO:Calculating mean and std
2024-11-24 17:46:20,408:INFO:Creating metrics dataframe
2024-11-24 17:46:20,413:INFO:Uploading results into container
2024-11-24 17:46:20,414:INFO:Uploading model into container now
2024-11-24 17:46:20,415:INFO:_master_model_container: 3
2024-11-24 17:46:20,415:INFO:_display_container: 2
2024-11-24 17:46:20,416:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-24 17:46:20,416:INFO:create_model() successfully completed......................................
2024-11-24 17:46:20,511:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:20,512:INFO:Creating metrics dataframe
2024-11-24 17:46:20,530:INFO:Initializing Decision Tree Classifier
2024-11-24 17:46:20,531:INFO:Total runtime is 0.26996353864669803 minutes
2024-11-24 17:46:20,540:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:20,541:INFO:Initializing create_model()
2024-11-24 17:46:20,542:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:20,543:INFO:Checking exceptions
2024-11-24 17:46:20,544:INFO:Importing libraries
2024-11-24 17:46:20,544:INFO:Copying training dataset
2024-11-24 17:46:20,565:INFO:Defining folds
2024-11-24 17:46:20,566:INFO:Declaring metric variables
2024-11-24 17:46:20,574:INFO:Importing untrained model
2024-11-24 17:46:20,581:INFO:Decision Tree Classifier Imported successfully
2024-11-24 17:46:20,592:INFO:Starting cross validation
2024-11-24 17:46:20,602:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:21,743:INFO:Calculating mean and std
2024-11-24 17:46:21,745:INFO:Creating metrics dataframe
2024-11-24 17:46:21,751:INFO:Uploading results into container
2024-11-24 17:46:21,753:INFO:Uploading model into container now
2024-11-24 17:46:21,754:INFO:_master_model_container: 4
2024-11-24 17:46:21,755:INFO:_display_container: 2
2024-11-24 17:46:21,756:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-24 17:46:21,757:INFO:create_model() successfully completed......................................
2024-11-24 17:46:21,848:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:21,850:INFO:Creating metrics dataframe
2024-11-24 17:46:21,871:INFO:Initializing SVM - Linear Kernel
2024-11-24 17:46:21,872:INFO:Total runtime is 0.29231150150299073 minutes
2024-11-24 17:46:21,880:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:21,882:INFO:Initializing create_model()
2024-11-24 17:46:21,882:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:21,883:INFO:Checking exceptions
2024-11-24 17:46:21,883:INFO:Importing libraries
2024-11-24 17:46:21,884:INFO:Copying training dataset
2024-11-24 17:46:21,902:INFO:Defining folds
2024-11-24 17:46:21,903:INFO:Declaring metric variables
2024-11-24 17:46:21,908:INFO:Importing untrained model
2024-11-24 17:46:21,914:INFO:SVM - Linear Kernel Imported successfully
2024-11-24 17:46:21,929:INFO:Starting cross validation
2024-11-24 17:46:21,935:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:23,246:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,321:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,390:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,399:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,416:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,437:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,450:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:23,469:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,477:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:23,493:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,493:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,501:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:23,510:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:23,520:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:23,539:INFO:Calculating mean and std
2024-11-24 17:46:23,542:INFO:Creating metrics dataframe
2024-11-24 17:46:23,546:INFO:Uploading results into container
2024-11-24 17:46:23,548:INFO:Uploading model into container now
2024-11-24 17:46:23,549:INFO:_master_model_container: 5
2024-11-24 17:46:23,549:INFO:_display_container: 2
2024-11-24 17:46:23,550:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-24 17:46:23,551:INFO:create_model() successfully completed......................................
2024-11-24 17:46:23,650:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:23,652:INFO:Creating metrics dataframe
2024-11-24 17:46:23,673:INFO:Initializing Ridge Classifier
2024-11-24 17:46:23,674:INFO:Total runtime is 0.32234501441319785 minutes
2024-11-24 17:46:23,683:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:23,685:INFO:Initializing create_model()
2024-11-24 17:46:23,685:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:23,686:INFO:Checking exceptions
2024-11-24 17:46:23,687:INFO:Importing libraries
2024-11-24 17:46:23,687:INFO:Copying training dataset
2024-11-24 17:46:23,714:INFO:Defining folds
2024-11-24 17:46:23,715:INFO:Declaring metric variables
2024-11-24 17:46:23,725:INFO:Importing untrained model
2024-11-24 17:46:23,731:INFO:Ridge Classifier Imported successfully
2024-11-24 17:46:23,749:INFO:Starting cross validation
2024-11-24 17:46:23,758:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:24,727:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,740:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,764:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,765:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,771:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,779:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,780:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,785:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,793:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,799:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,816:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,818:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,820:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,825:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,826:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,827:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,839:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,846:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,853:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:24,860:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:24,874:INFO:Calculating mean and std
2024-11-24 17:46:24,876:INFO:Creating metrics dataframe
2024-11-24 17:46:24,879:INFO:Uploading results into container
2024-11-24 17:46:24,881:INFO:Uploading model into container now
2024-11-24 17:46:24,882:INFO:_master_model_container: 6
2024-11-24 17:46:24,882:INFO:_display_container: 2
2024-11-24 17:46:24,883:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-24 17:46:24,883:INFO:create_model() successfully completed......................................
2024-11-24 17:46:24,977:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:24,978:INFO:Creating metrics dataframe
2024-11-24 17:46:24,996:INFO:Initializing Random Forest Classifier
2024-11-24 17:46:24,997:INFO:Total runtime is 0.34439792633056643 minutes
2024-11-24 17:46:25,005:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:25,006:INFO:Initializing create_model()
2024-11-24 17:46:25,007:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:25,008:INFO:Checking exceptions
2024-11-24 17:46:25,009:INFO:Importing libraries
2024-11-24 17:46:25,010:INFO:Copying training dataset
2024-11-24 17:46:25,028:INFO:Defining folds
2024-11-24 17:46:25,029:INFO:Declaring metric variables
2024-11-24 17:46:25,040:INFO:Importing untrained model
2024-11-24 17:46:25,049:INFO:Random Forest Classifier Imported successfully
2024-11-24 17:46:25,064:INFO:Starting cross validation
2024-11-24 17:46:25,076:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:28,226:INFO:Calculating mean and std
2024-11-24 17:46:28,228:INFO:Creating metrics dataframe
2024-11-24 17:46:28,232:INFO:Uploading results into container
2024-11-24 17:46:28,233:INFO:Uploading model into container now
2024-11-24 17:46:28,234:INFO:_master_model_container: 7
2024-11-24 17:46:28,235:INFO:_display_container: 2
2024-11-24 17:46:28,236:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-24 17:46:28,237:INFO:create_model() successfully completed......................................
2024-11-24 17:46:28,312:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:28,313:INFO:Creating metrics dataframe
2024-11-24 17:46:28,327:INFO:Initializing Quadratic Discriminant Analysis
2024-11-24 17:46:28,328:INFO:Total runtime is 0.3999203085899353 minutes
2024-11-24 17:46:28,333:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:28,334:INFO:Initializing create_model()
2024-11-24 17:46:28,335:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:28,335:INFO:Checking exceptions
2024-11-24 17:46:28,336:INFO:Importing libraries
2024-11-24 17:46:28,336:INFO:Copying training dataset
2024-11-24 17:46:28,348:INFO:Defining folds
2024-11-24 17:46:28,348:INFO:Declaring metric variables
2024-11-24 17:46:28,356:INFO:Importing untrained model
2024-11-24 17:46:28,362:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-24 17:46:28,374:INFO:Starting cross validation
2024-11-24 17:46:28,379:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:29,223:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,257:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,274:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,290:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,314:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,314:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,329:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,338:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,359:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,391:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 17:46:29,459:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,470:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:29,488:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,498:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,512:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,515:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,541:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,543:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,546:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,574:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,579:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:29,586:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 17:46:29,612:INFO:Calculating mean and std
2024-11-24 17:46:29,615:INFO:Creating metrics dataframe
2024-11-24 17:46:29,620:INFO:Uploading results into container
2024-11-24 17:46:29,621:INFO:Uploading model into container now
2024-11-24 17:46:29,622:INFO:_master_model_container: 8
2024-11-24 17:46:29,623:INFO:_display_container: 2
2024-11-24 17:46:29,624:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-24 17:46:29,624:INFO:create_model() successfully completed......................................
2024-11-24 17:46:29,722:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:29,723:INFO:Creating metrics dataframe
2024-11-24 17:46:29,741:INFO:Initializing Ada Boost Classifier
2024-11-24 17:46:29,742:INFO:Total runtime is 0.4234880844751994 minutes
2024-11-24 17:46:29,751:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:29,752:INFO:Initializing create_model()
2024-11-24 17:46:29,753:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:29,753:INFO:Checking exceptions
2024-11-24 17:46:29,754:INFO:Importing libraries
2024-11-24 17:46:29,755:INFO:Copying training dataset
2024-11-24 17:46:29,774:INFO:Defining folds
2024-11-24 17:46:29,775:INFO:Declaring metric variables
2024-11-24 17:46:29,782:INFO:Importing untrained model
2024-11-24 17:46:29,790:INFO:Ada Boost Classifier Imported successfully
2024-11-24 17:46:29,807:INFO:Starting cross validation
2024-11-24 17:46:29,814:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:30,530:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:30,532:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:30,553:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:30,562:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:30,580:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:30,585:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:30,594:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:30,614:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:30,618:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:30,628:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 17:46:31,565:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,586:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,590:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,604:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,614:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,624:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,631:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,643:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,649:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,659:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:31,693:INFO:Calculating mean and std
2024-11-24 17:46:31,696:INFO:Creating metrics dataframe
2024-11-24 17:46:31,700:INFO:Uploading results into container
2024-11-24 17:46:31,702:INFO:Uploading model into container now
2024-11-24 17:46:31,703:INFO:_master_model_container: 9
2024-11-24 17:46:31,704:INFO:_display_container: 2
2024-11-24 17:46:31,705:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-24 17:46:31,705:INFO:create_model() successfully completed......................................
2024-11-24 17:46:31,803:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:31,805:INFO:Creating metrics dataframe
2024-11-24 17:46:31,828:INFO:Initializing Gradient Boosting Classifier
2024-11-24 17:46:31,829:INFO:Total runtime is 0.4582671920458476 minutes
2024-11-24 17:46:31,836:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:31,837:INFO:Initializing create_model()
2024-11-24 17:46:31,838:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:31,839:INFO:Checking exceptions
2024-11-24 17:46:31,841:INFO:Importing libraries
2024-11-24 17:46:31,842:INFO:Copying training dataset
2024-11-24 17:46:31,857:INFO:Defining folds
2024-11-24 17:46:31,858:INFO:Declaring metric variables
2024-11-24 17:46:31,865:INFO:Importing untrained model
2024-11-24 17:46:31,873:INFO:Gradient Boosting Classifier Imported successfully
2024-11-24 17:46:31,885:INFO:Starting cross validation
2024-11-24 17:46:31,892:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:43,981:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:43,993:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:44,074:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:44,099:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:44,123:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:44,163:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:44,183:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:44,184:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:44,211:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:44,261:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:44,293:INFO:Calculating mean and std
2024-11-24 17:46:44,296:INFO:Creating metrics dataframe
2024-11-24 17:46:44,302:INFO:Uploading results into container
2024-11-24 17:46:44,303:INFO:Uploading model into container now
2024-11-24 17:46:44,304:INFO:_master_model_container: 10
2024-11-24 17:46:44,305:INFO:_display_container: 2
2024-11-24 17:46:44,306:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-24 17:46:44,306:INFO:create_model() successfully completed......................................
2024-11-24 17:46:44,402:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:44,402:INFO:Creating metrics dataframe
2024-11-24 17:46:44,421:INFO:Initializing Linear Discriminant Analysis
2024-11-24 17:46:44,422:INFO:Total runtime is 0.6681528687477112 minutes
2024-11-24 17:46:44,431:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:44,432:INFO:Initializing create_model()
2024-11-24 17:46:44,434:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:44,437:INFO:Checking exceptions
2024-11-24 17:46:44,438:INFO:Importing libraries
2024-11-24 17:46:44,438:INFO:Copying training dataset
2024-11-24 17:46:44,465:INFO:Defining folds
2024-11-24 17:46:44,469:INFO:Declaring metric variables
2024-11-24 17:46:44,476:INFO:Importing untrained model
2024-11-24 17:46:44,491:INFO:Linear Discriminant Analysis Imported successfully
2024-11-24 17:46:44,503:INFO:Starting cross validation
2024-11-24 17:46:44,509:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:45,599:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,607:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,615:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,646:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,652:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,664:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,665:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,669:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,709:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,711:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 17:46:45,731:INFO:Calculating mean and std
2024-11-24 17:46:45,733:INFO:Creating metrics dataframe
2024-11-24 17:46:45,737:INFO:Uploading results into container
2024-11-24 17:46:45,739:INFO:Uploading model into container now
2024-11-24 17:46:45,740:INFO:_master_model_container: 11
2024-11-24 17:46:45,740:INFO:_display_container: 2
2024-11-24 17:46:45,741:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-24 17:46:45,742:INFO:create_model() successfully completed......................................
2024-11-24 17:46:45,840:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:45,841:INFO:Creating metrics dataframe
2024-11-24 17:46:45,867:INFO:Initializing Extra Trees Classifier
2024-11-24 17:46:45,869:INFO:Total runtime is 0.6922608772913615 minutes
2024-11-24 17:46:45,875:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:45,877:INFO:Initializing create_model()
2024-11-24 17:46:45,878:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:45,878:INFO:Checking exceptions
2024-11-24 17:46:45,880:INFO:Importing libraries
2024-11-24 17:46:45,880:INFO:Copying training dataset
2024-11-24 17:46:45,894:INFO:Defining folds
2024-11-24 17:46:45,896:INFO:Declaring metric variables
2024-11-24 17:46:45,904:INFO:Importing untrained model
2024-11-24 17:46:45,910:INFO:Extra Trees Classifier Imported successfully
2024-11-24 17:46:45,928:INFO:Starting cross validation
2024-11-24 17:46:45,939:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 17:46:53,630:INFO:Calculating mean and std
2024-11-24 17:46:53,633:INFO:Creating metrics dataframe
2024-11-24 17:46:53,639:INFO:Uploading results into container
2024-11-24 17:46:53,642:INFO:Uploading model into container now
2024-11-24 17:46:53,644:INFO:_master_model_container: 12
2024-11-24 17:46:53,645:INFO:_display_container: 2
2024-11-24 17:46:53,647:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-24 17:46:53,648:INFO:create_model() successfully completed......................................
2024-11-24 17:46:53,753:INFO:SubProcess create_model() end ==================================
2024-11-24 17:46:53,754:INFO:Creating metrics dataframe
2024-11-24 17:46:53,776:INFO:Initializing Light Gradient Boosting Machine
2024-11-24 17:46:53,777:INFO:Total runtime is 0.8240631540616354 minutes
2024-11-24 17:46:53,782:INFO:SubProcess create_model() called ==================================
2024-11-24 17:46:53,783:INFO:Initializing create_model()
2024-11-24 17:46:53,783:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3282f4f010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 17:46:53,783:INFO:Checking exceptions
2024-11-24 17:46:53,784:INFO:Importing libraries
2024-11-24 17:46:53,785:INFO:Copying training dataset
2024-11-24 17:46:53,796:INFO:Defining folds
2024-11-24 17:46:53,797:INFO:Declaring metric variables
2024-11-24 17:46:53,804:INFO:Importing untrained model
2024-11-24 17:46:53,811:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-24 17:46:53,824:INFO:Starting cross validation
2024-11-24 17:46:53,830:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:36,492:INFO:Initializing compare_models()
2024-11-24 18:19:36,494:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, 'include': None, 'exclude': ['lightgbm'], 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=['lightgbm'])
2024-11-24 18:19:36,494:INFO:Checking exceptions
2024-11-24 18:19:36,522:INFO:Preparing display monitor
2024-11-24 18:19:36,563:INFO:Initializing Logistic Regression
2024-11-24 18:19:36,564:INFO:Total runtime is 1.180569330851237e-05 minutes
2024-11-24 18:19:36,568:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:36,570:INFO:Initializing create_model()
2024-11-24 18:19:36,570:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:36,571:INFO:Checking exceptions
2024-11-24 18:19:36,571:INFO:Importing libraries
2024-11-24 18:19:36,571:INFO:Copying training dataset
2024-11-24 18:19:36,580:INFO:Defining folds
2024-11-24 18:19:36,581:INFO:Declaring metric variables
2024-11-24 18:19:36,586:INFO:Importing untrained model
2024-11-24 18:19:36,591:INFO:Logistic Regression Imported successfully
2024-11-24 18:19:36,598:INFO:Starting cross validation
2024-11-24 18:19:36,607:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:44,617:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,632:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,663:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,708:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,714:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,725:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,727:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,734:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:44,742:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,742:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,753:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,755:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,761:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,761:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:44,768:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-24 18:19:44,821:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,821:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,823:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,828:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:44,830:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:44,830:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:44,832:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,846:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,850:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,853:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:44,858:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:44,860:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:44,880:INFO:Calculating mean and std
2024-11-24 18:19:44,885:INFO:Creating metrics dataframe
2024-11-24 18:19:44,893:INFO:Uploading results into container
2024-11-24 18:19:44,896:INFO:Uploading model into container now
2024-11-24 18:19:44,899:INFO:_master_model_container: 13
2024-11-24 18:19:44,900:INFO:_display_container: 2
2024-11-24 18:19:44,902:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-24 18:19:44,903:INFO:create_model() successfully completed......................................
2024-11-24 18:19:45,100:INFO:SubProcess create_model() end ==================================
2024-11-24 18:19:45,101:INFO:Creating metrics dataframe
2024-11-24 18:19:45,109:INFO:Initializing K Neighbors Classifier
2024-11-24 18:19:45,110:INFO:Total runtime is 0.14243896007537843 minutes
2024-11-24 18:19:45,113:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:45,115:INFO:Initializing create_model()
2024-11-24 18:19:45,116:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:45,117:INFO:Checking exceptions
2024-11-24 18:19:45,117:INFO:Importing libraries
2024-11-24 18:19:45,118:INFO:Copying training dataset
2024-11-24 18:19:45,137:INFO:Defining folds
2024-11-24 18:19:45,138:INFO:Declaring metric variables
2024-11-24 18:19:45,145:INFO:Importing untrained model
2024-11-24 18:19:45,151:INFO:K Neighbors Classifier Imported successfully
2024-11-24 18:19:45,160:INFO:Starting cross validation
2024-11-24 18:19:45,164:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:50,335:INFO:Calculating mean and std
2024-11-24 18:19:50,346:INFO:Creating metrics dataframe
2024-11-24 18:19:50,368:INFO:Uploading results into container
2024-11-24 18:19:50,370:INFO:Uploading model into container now
2024-11-24 18:19:50,372:INFO:_master_model_container: 14
2024-11-24 18:19:50,374:INFO:_display_container: 2
2024-11-24 18:19:50,378:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-24 18:19:50,378:INFO:create_model() successfully completed......................................
2024-11-24 18:19:50,657:INFO:SubProcess create_model() end ==================================
2024-11-24 18:19:50,658:INFO:Creating metrics dataframe
2024-11-24 18:19:50,669:INFO:Initializing Naive Bayes
2024-11-24 18:19:50,670:INFO:Total runtime is 0.2351166884104411 minutes
2024-11-24 18:19:50,674:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:50,675:INFO:Initializing create_model()
2024-11-24 18:19:50,676:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:50,677:INFO:Checking exceptions
2024-11-24 18:19:50,677:INFO:Importing libraries
2024-11-24 18:19:50,678:INFO:Copying training dataset
2024-11-24 18:19:50,687:INFO:Defining folds
2024-11-24 18:19:50,687:INFO:Declaring metric variables
2024-11-24 18:19:50,691:INFO:Importing untrained model
2024-11-24 18:19:50,697:INFO:Naive Bayes Imported successfully
2024-11-24 18:19:50,706:INFO:Starting cross validation
2024-11-24 18:19:50,711:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:51,562:INFO:Calculating mean and std
2024-11-24 18:19:51,563:INFO:Creating metrics dataframe
2024-11-24 18:19:51,566:INFO:Uploading results into container
2024-11-24 18:19:51,567:INFO:Uploading model into container now
2024-11-24 18:19:51,567:INFO:_master_model_container: 15
2024-11-24 18:19:51,568:INFO:_display_container: 2
2024-11-24 18:19:51,568:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-24 18:19:51,569:INFO:create_model() successfully completed......................................
2024-11-24 18:19:51,654:INFO:SubProcess create_model() end ==================================
2024-11-24 18:19:51,655:INFO:Creating metrics dataframe
2024-11-24 18:19:51,665:INFO:Initializing Decision Tree Classifier
2024-11-24 18:19:51,666:INFO:Total runtime is 0.25170486768086753 minutes
2024-11-24 18:19:51,669:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:51,671:INFO:Initializing create_model()
2024-11-24 18:19:51,672:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:51,672:INFO:Checking exceptions
2024-11-24 18:19:51,673:INFO:Importing libraries
2024-11-24 18:19:51,674:INFO:Copying training dataset
2024-11-24 18:19:51,681:INFO:Defining folds
2024-11-24 18:19:51,683:INFO:Declaring metric variables
2024-11-24 18:19:51,687:INFO:Importing untrained model
2024-11-24 18:19:51,693:INFO:Decision Tree Classifier Imported successfully
2024-11-24 18:19:51,703:INFO:Starting cross validation
2024-11-24 18:19:51,707:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:52,494:INFO:Calculating mean and std
2024-11-24 18:19:52,496:INFO:Creating metrics dataframe
2024-11-24 18:19:52,499:INFO:Uploading results into container
2024-11-24 18:19:52,501:INFO:Uploading model into container now
2024-11-24 18:19:52,502:INFO:_master_model_container: 16
2024-11-24 18:19:52,502:INFO:_display_container: 2
2024-11-24 18:19:52,503:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-24 18:19:52,504:INFO:create_model() successfully completed......................................
2024-11-24 18:19:52,583:INFO:SubProcess create_model() end ==================================
2024-11-24 18:19:52,584:INFO:Creating metrics dataframe
2024-11-24 18:19:52,590:INFO:Initializing SVM - Linear Kernel
2024-11-24 18:19:52,591:INFO:Total runtime is 0.26712541182835897 minutes
2024-11-24 18:19:52,595:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:52,596:INFO:Initializing create_model()
2024-11-24 18:19:52,597:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:52,598:INFO:Checking exceptions
2024-11-24 18:19:52,599:INFO:Importing libraries
2024-11-24 18:19:52,599:INFO:Copying training dataset
2024-11-24 18:19:52,607:INFO:Defining folds
2024-11-24 18:19:52,609:INFO:Declaring metric variables
2024-11-24 18:19:52,615:INFO:Importing untrained model
2024-11-24 18:19:52,620:INFO:SVM - Linear Kernel Imported successfully
2024-11-24 18:19:52,629:INFO:Starting cross validation
2024-11-24 18:19:52,633:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:53,505:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,512:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:53,516:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,525:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,535:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,572:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,578:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:53,587:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,599:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,601:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,603:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:53,605:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:53,614:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,632:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:53,645:INFO:Calculating mean and std
2024-11-24 18:19:53,647:INFO:Creating metrics dataframe
2024-11-24 18:19:53,649:INFO:Uploading results into container
2024-11-24 18:19:53,651:INFO:Uploading model into container now
2024-11-24 18:19:53,652:INFO:_master_model_container: 17
2024-11-24 18:19:53,653:INFO:_display_container: 2
2024-11-24 18:19:53,654:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-24 18:19:53,654:INFO:create_model() successfully completed......................................
2024-11-24 18:19:53,736:INFO:SubProcess create_model() end ==================================
2024-11-24 18:19:53,737:INFO:Creating metrics dataframe
2024-11-24 18:19:53,746:INFO:Initializing Ridge Classifier
2024-11-24 18:19:53,747:INFO:Total runtime is 0.2863870183626811 minutes
2024-11-24 18:19:53,751:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:53,752:INFO:Initializing create_model()
2024-11-24 18:19:53,753:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:53,753:INFO:Checking exceptions
2024-11-24 18:19:53,754:INFO:Importing libraries
2024-11-24 18:19:53,755:INFO:Copying training dataset
2024-11-24 18:19:53,765:INFO:Defining folds
2024-11-24 18:19:53,766:INFO:Declaring metric variables
2024-11-24 18:19:53,770:INFO:Importing untrained model
2024-11-24 18:19:53,773:INFO:Ridge Classifier Imported successfully
2024-11-24 18:19:53,781:INFO:Starting cross validation
2024-11-24 18:19:53,785:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:54,404:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,416:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,425:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,435:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,436:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,445:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,450:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,451:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,457:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,458:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,476:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,479:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,483:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,486:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,486:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,490:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,491:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,495:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,502:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:54,506:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:54,513:INFO:Calculating mean and std
2024-11-24 18:19:54,515:INFO:Creating metrics dataframe
2024-11-24 18:19:54,518:INFO:Uploading results into container
2024-11-24 18:19:54,519:INFO:Uploading model into container now
2024-11-24 18:19:54,520:INFO:_master_model_container: 18
2024-11-24 18:19:54,520:INFO:_display_container: 2
2024-11-24 18:19:54,521:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-24 18:19:54,522:INFO:create_model() successfully completed......................................
2024-11-24 18:19:54,606:INFO:SubProcess create_model() end ==================================
2024-11-24 18:19:54,606:INFO:Creating metrics dataframe
2024-11-24 18:19:54,615:INFO:Initializing Random Forest Classifier
2024-11-24 18:19:54,616:INFO:Total runtime is 0.30087693532307946 minutes
2024-11-24 18:19:54,620:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:54,621:INFO:Initializing create_model()
2024-11-24 18:19:54,621:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:54,622:INFO:Checking exceptions
2024-11-24 18:19:54,623:INFO:Importing libraries
2024-11-24 18:19:54,623:INFO:Copying training dataset
2024-11-24 18:19:54,632:INFO:Defining folds
2024-11-24 18:19:54,634:INFO:Declaring metric variables
2024-11-24 18:19:54,638:INFO:Importing untrained model
2024-11-24 18:19:54,643:INFO:Random Forest Classifier Imported successfully
2024-11-24 18:19:54,653:INFO:Starting cross validation
2024-11-24 18:19:54,657:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:57,037:INFO:Calculating mean and std
2024-11-24 18:19:57,039:INFO:Creating metrics dataframe
2024-11-24 18:19:57,041:INFO:Uploading results into container
2024-11-24 18:19:57,043:INFO:Uploading model into container now
2024-11-24 18:19:57,044:INFO:_master_model_container: 19
2024-11-24 18:19:57,044:INFO:_display_container: 2
2024-11-24 18:19:57,045:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-24 18:19:57,046:INFO:create_model() successfully completed......................................
2024-11-24 18:19:57,129:INFO:SubProcess create_model() end ==================================
2024-11-24 18:19:57,130:INFO:Creating metrics dataframe
2024-11-24 18:19:57,138:INFO:Initializing Quadratic Discriminant Analysis
2024-11-24 18:19:57,140:INFO:Total runtime is 0.3429470539093018 minutes
2024-11-24 18:19:57,144:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:57,145:INFO:Initializing create_model()
2024-11-24 18:19:57,146:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:57,146:INFO:Checking exceptions
2024-11-24 18:19:57,147:INFO:Importing libraries
2024-11-24 18:19:57,148:INFO:Copying training dataset
2024-11-24 18:19:57,156:INFO:Defining folds
2024-11-24 18:19:57,157:INFO:Declaring metric variables
2024-11-24 18:19:57,161:INFO:Importing untrained model
2024-11-24 18:19:57,166:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-24 18:19:57,174:INFO:Starting cross validation
2024-11-24 18:19:57,178:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:57,759:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,759:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,759:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,765:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,774:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,792:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,810:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,820:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,826:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,846:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-24 18:19:57,924:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:57,927:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:57,927:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:57,928:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:57,932:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:57,936:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:19:57,939:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:57,944:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:57,953:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:57,974:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:57,981:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:57,993:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:58,006:INFO:Calculating mean and std
2024-11-24 18:19:58,008:INFO:Creating metrics dataframe
2024-11-24 18:19:58,011:INFO:Uploading results into container
2024-11-24 18:19:58,012:INFO:Uploading model into container now
2024-11-24 18:19:58,013:INFO:_master_model_container: 20
2024-11-24 18:19:58,014:INFO:_display_container: 2
2024-11-24 18:19:58,014:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-24 18:19:58,015:INFO:create_model() successfully completed......................................
2024-11-24 18:19:58,092:INFO:SubProcess create_model() end ==================================
2024-11-24 18:19:58,093:INFO:Creating metrics dataframe
2024-11-24 18:19:58,102:INFO:Initializing Ada Boost Classifier
2024-11-24 18:19:58,102:INFO:Total runtime is 0.3589798967043559 minutes
2024-11-24 18:19:58,106:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:58,107:INFO:Initializing create_model()
2024-11-24 18:19:58,108:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:58,108:INFO:Checking exceptions
2024-11-24 18:19:58,109:INFO:Importing libraries
2024-11-24 18:19:58,109:INFO:Copying training dataset
2024-11-24 18:19:58,118:INFO:Defining folds
2024-11-24 18:19:58,119:INFO:Declaring metric variables
2024-11-24 18:19:58,124:INFO:Importing untrained model
2024-11-24 18:19:58,130:INFO:Ada Boost Classifier Imported successfully
2024-11-24 18:19:58,137:INFO:Starting cross validation
2024-11-24 18:19:58,141:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:19:58,684:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:58,695:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:58,703:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:58,707:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:58,709:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:58,713:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:58,714:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:58,744:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:58,745:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:58,746:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-24 18:19:59,450:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,471:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,485:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,492:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,495:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,497:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,511:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,520:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,526:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,529:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:19:59,543:INFO:Calculating mean and std
2024-11-24 18:19:59,545:INFO:Creating metrics dataframe
2024-11-24 18:19:59,548:INFO:Uploading results into container
2024-11-24 18:19:59,549:INFO:Uploading model into container now
2024-11-24 18:19:59,551:INFO:_master_model_container: 21
2024-11-24 18:19:59,551:INFO:_display_container: 2
2024-11-24 18:19:59,552:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-24 18:19:59,553:INFO:create_model() successfully completed......................................
2024-11-24 18:19:59,623:INFO:SubProcess create_model() end ==================================
2024-11-24 18:19:59,624:INFO:Creating metrics dataframe
2024-11-24 18:19:59,631:INFO:Initializing Gradient Boosting Classifier
2024-11-24 18:19:59,632:INFO:Total runtime is 0.3844803214073182 minutes
2024-11-24 18:19:59,636:INFO:SubProcess create_model() called ==================================
2024-11-24 18:19:59,637:INFO:Initializing create_model()
2024-11-24 18:19:59,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:19:59,639:INFO:Checking exceptions
2024-11-24 18:19:59,640:INFO:Importing libraries
2024-11-24 18:19:59,641:INFO:Copying training dataset
2024-11-24 18:19:59,651:INFO:Defining folds
2024-11-24 18:19:59,652:INFO:Declaring metric variables
2024-11-24 18:19:59,656:INFO:Importing untrained model
2024-11-24 18:19:59,660:INFO:Gradient Boosting Classifier Imported successfully
2024-11-24 18:19:59,668:INFO:Starting cross validation
2024-11-24 18:19:59,673:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:20:07,855:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:07,869:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:07,877:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:07,934:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:07,956:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:07,972:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:08,016:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:08,049:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:08,068:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:08,243:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:08,259:INFO:Calculating mean and std
2024-11-24 18:20:08,261:INFO:Creating metrics dataframe
2024-11-24 18:20:08,264:INFO:Uploading results into container
2024-11-24 18:20:08,265:INFO:Uploading model into container now
2024-11-24 18:20:08,266:INFO:_master_model_container: 22
2024-11-24 18:20:08,267:INFO:_display_container: 2
2024-11-24 18:20:08,269:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-24 18:20:08,269:INFO:create_model() successfully completed......................................
2024-11-24 18:20:08,335:INFO:SubProcess create_model() end ==================================
2024-11-24 18:20:08,336:INFO:Creating metrics dataframe
2024-11-24 18:20:08,344:INFO:Initializing Linear Discriminant Analysis
2024-11-24 18:20:08,345:INFO:Total runtime is 0.529692852497101 minutes
2024-11-24 18:20:08,349:INFO:SubProcess create_model() called ==================================
2024-11-24 18:20:08,350:INFO:Initializing create_model()
2024-11-24 18:20:08,351:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:20:08,352:INFO:Checking exceptions
2024-11-24 18:20:08,353:INFO:Importing libraries
2024-11-24 18:20:08,354:INFO:Copying training dataset
2024-11-24 18:20:08,361:INFO:Defining folds
2024-11-24 18:20:08,362:INFO:Declaring metric variables
2024-11-24 18:20:08,367:INFO:Importing untrained model
2024-11-24 18:20:08,378:INFO:Linear Discriminant Analysis Imported successfully
2024-11-24 18:20:08,391:INFO:Starting cross validation
2024-11-24 18:20:08,395:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:20:09,017:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,032:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,038:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,072:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,078:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,080:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,082:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,089:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,116:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,124:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-24 18:20:09,137:INFO:Calculating mean and std
2024-11-24 18:20:09,139:INFO:Creating metrics dataframe
2024-11-24 18:20:09,142:INFO:Uploading results into container
2024-11-24 18:20:09,144:INFO:Uploading model into container now
2024-11-24 18:20:09,144:INFO:_master_model_container: 23
2024-11-24 18:20:09,145:INFO:_display_container: 2
2024-11-24 18:20:09,146:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-24 18:20:09,147:INFO:create_model() successfully completed......................................
2024-11-24 18:20:09,215:INFO:SubProcess create_model() end ==================================
2024-11-24 18:20:09,216:INFO:Creating metrics dataframe
2024-11-24 18:20:09,224:INFO:Initializing Extra Trees Classifier
2024-11-24 18:20:09,225:INFO:Total runtime is 0.5443661411603293 minutes
2024-11-24 18:20:09,229:INFO:SubProcess create_model() called ==================================
2024-11-24 18:20:09,231:INFO:Initializing create_model()
2024-11-24 18:20:09,232:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:20:09,232:INFO:Checking exceptions
2024-11-24 18:20:09,233:INFO:Importing libraries
2024-11-24 18:20:09,234:INFO:Copying training dataset
2024-11-24 18:20:09,241:INFO:Defining folds
2024-11-24 18:20:09,242:INFO:Declaring metric variables
2024-11-24 18:20:09,246:INFO:Importing untrained model
2024-11-24 18:20:09,252:INFO:Extra Trees Classifier Imported successfully
2024-11-24 18:20:09,260:INFO:Starting cross validation
2024-11-24 18:20:09,264:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:20:13,662:INFO:Calculating mean and std
2024-11-24 18:20:13,665:INFO:Creating metrics dataframe
2024-11-24 18:20:13,672:INFO:Uploading results into container
2024-11-24 18:20:13,674:INFO:Uploading model into container now
2024-11-24 18:20:13,675:INFO:_master_model_container: 24
2024-11-24 18:20:13,676:INFO:_display_container: 2
2024-11-24 18:20:13,678:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-24 18:20:13,678:INFO:create_model() successfully completed......................................
2024-11-24 18:20:13,793:INFO:SubProcess create_model() end ==================================
2024-11-24 18:20:13,794:INFO:Creating metrics dataframe
2024-11-24 18:20:13,806:INFO:Initializing Dummy Classifier
2024-11-24 18:20:13,806:INFO:Total runtime is 0.620716424783071 minutes
2024-11-24 18:20:13,811:INFO:SubProcess create_model() called ==================================
2024-11-24 18:20:13,812:INFO:Initializing create_model()
2024-11-24 18:20:13,813:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f32822eec50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:20:13,813:INFO:Checking exceptions
2024-11-24 18:20:13,814:INFO:Importing libraries
2024-11-24 18:20:13,815:INFO:Copying training dataset
2024-11-24 18:20:13,825:INFO:Defining folds
2024-11-24 18:20:13,826:INFO:Declaring metric variables
2024-11-24 18:20:13,831:INFO:Importing untrained model
2024-11-24 18:20:13,836:INFO:Dummy Classifier Imported successfully
2024-11-24 18:20:13,846:INFO:Starting cross validation
2024-11-24 18:20:13,850:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-24 18:20:14,565:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,594:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,608:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,622:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,627:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,652:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,674:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,675:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,692:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,703:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-24 18:20:14,720:INFO:Calculating mean and std
2024-11-24 18:20:14,722:INFO:Creating metrics dataframe
2024-11-24 18:20:14,724:INFO:Uploading results into container
2024-11-24 18:20:14,725:INFO:Uploading model into container now
2024-11-24 18:20:14,726:INFO:_master_model_container: 25
2024-11-24 18:20:14,727:INFO:_display_container: 2
2024-11-24 18:20:14,728:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-24 18:20:14,729:INFO:create_model() successfully completed......................................
2024-11-24 18:20:14,811:INFO:SubProcess create_model() end ==================================
2024-11-24 18:20:14,812:INFO:Creating metrics dataframe
2024-11-24 18:20:14,822:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-11-24 18:20:14,834:INFO:Initializing create_model()
2024-11-24 18:20:14,834:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:20:14,835:INFO:Checking exceptions
2024-11-24 18:20:14,843:INFO:Importing libraries
2024-11-24 18:20:14,844:INFO:Copying training dataset
2024-11-24 18:20:14,851:INFO:Defining folds
2024-11-24 18:20:14,852:INFO:Declaring metric variables
2024-11-24 18:20:14,853:INFO:Importing untrained model
2024-11-24 18:20:14,853:INFO:Declaring custom model
2024-11-24 18:20:14,855:INFO:Ridge Classifier Imported successfully
2024-11-24 18:20:14,860:INFO:Cross validation set to False
2024-11-24 18:20:14,862:INFO:Fitting Model
2024-11-24 18:20:15,234:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-24 18:20:15,235:INFO:create_model() successfully completed......................................
2024-11-24 18:20:15,394:INFO:_master_model_container: 25
2024-11-24 18:20:15,395:INFO:_display_container: 2
2024-11-24 18:20:15,396:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-24 18:20:15,396:INFO:compare_models() successfully completed......................................
2024-11-24 18:20:57,805:INFO:Initializing create_model()
2024-11-24 18:20:57,806:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f3282f4ece0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-24 18:20:57,808:INFO:Checking exceptions
2024-11-24 18:20:57,839:INFO:Importing libraries
2024-11-24 18:20:57,840:INFO:Copying training dataset
2024-11-24 18:20:57,855:INFO:Defining folds
2024-11-24 18:20:57,856:INFO:Declaring metric variables
2024-11-24 18:20:57,863:INFO:Importing untrained model
2024-11-24 18:20:57,867:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-24 18:20:57,873:INFO:Starting cross validation
2024-11-24 18:20:57,878:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
